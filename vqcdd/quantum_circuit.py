"""
Quantum Circuit Module for VQCdd

This module implements configurable Variational Quantum Circuits (VQCs) for
pharmacokinetic parameter estimation. The design emphasizes simplicity,
explainability, and modularity for scientific research.

Key Features:
- Multiple ansatz options (RY-CNOT, Strongly Entangling)
- Flexible data encoding strategies
- PennyLane integration with clear interfaces
- Comprehensive documentation for research transparency
"""

import numpy as np
import pennylane as qml
from pennylane import numpy as qnp  # PennyLane numpy for trainable parameters
from typing import Dict, List, Optional, Tuple, Union, Callable, Any
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from collections import defaultdict
import matplotlib.pyplot as plt
import warnings


@dataclass
class CircuitConfig:
    """Configuration for quantum circuit design"""
    n_qubits: int = 6                           # Number of qubits (increased for 11 features)
    n_layers: int = 3                           # Circuit depth
    ansatz: str = "ry_cnot"                     # Ansatz type
    encoding: str = "angle"                     # Encoding type
    device_name: str = "default.qubit"          # PennyLane device
    shots: Optional[int] = None                 # None for exact simulation
    diff_method: str = "adjoint"              # Gradient computation method (better for barren plateaus)

    # Phase 2B: Adaptive circuit depth parameters
    adaptive_depth: bool = False                # Enable adaptive depth adjustment
    min_layers: int = 1                        # Minimum circuit depth
    max_layers: int = 10                       # Maximum circuit depth
    depth_adjustment_threshold: float = 0.01   # Gradient threshold for depth adjustment
    layer_wise_training: bool = False          # Enable layer-wise training

    # Phase 2B: Enhanced encoding parameters
    encoding_optimization: bool = False        # Enable data-adaptive encoding optimization
    feature_importance_analysis: bool = False  # Enable feature importance analysis
    encoding_comparison: bool = False          # Enable encoding strategy comparison

    # Phase 2B: Benchmarking parameters
    expressivity_benchmarking: bool = False    # Enable expressivity benchmarking
    performance_analysis: bool = False         # Enable performance analysis

    def __post_init__(self):
        """Validate configuration parameters"""
        assert self.n_qubits >= 2, "Need at least 2 qubits"
        assert self.n_layers >= 1, "Need at least 1 layer"

        # Extended ansatz validation for Phase 2B
        valid_ansatze = [
            "ry_cnot", "strongly_entangling", "hardware_efficient",
            "optimized_hardware_efficient", "qaoa_inspired", "pkpd_specific"
        ]
        assert self.ansatz in valid_ansatze, f"Invalid ansatz. Choose from {valid_ansatze}"

        # Extended encoding validation for Phase 2B
        valid_encodings = [
            "angle", "amplitude", "iqp", "basis", "displacement", "squeezing",
            "data_reuploading", "feature_map"
        ]
        assert self.encoding in valid_encodings, f"Invalid encoding. Choose from {valid_encodings}"

        # Adaptive depth validation
        if self.adaptive_depth:
            assert self.min_layers <= self.max_layers, "min_layers must be <= max_layers"
            assert self.min_layers >= 1, "min_layers must be >= 1"


class QuantumDataEncoder:
    """Enhanced quantum data encoder with multiple encoding strategies"""

    def __init__(self, encoding_type: str, n_qubits: int,
                 optimization_enabled: bool = False,
                 feature_importance_enabled: bool = False):
        self.encoding_type = encoding_type
        self.n_qubits = n_qubits
        self.optimization_enabled = optimization_enabled
        self.feature_importance_enabled = feature_importance_enabled

        # Phase 2B: Feature importance tracking
        self.feature_weights = None
        self.encoding_parameters = {}
        # Limit performance history to prevent memory issues
        from collections import deque
        self.performance_history = deque(maxlen=200)

    def encode(self, features: np.ndarray, encoding_params: Optional[Dict] = None) -> None:
        """
        Encode classical features into quantum state with enhanced strategies

        Args:
            features: Array of all 11 NONMEM features [ID, BW, COMED, DOSE, TIME, DV, EVID, MDV, AMT, CMT, DVID]
            encoding_params: Optional parameters for data-adaptive encoding
        """
        features = np.atleast_1d(features)
        n_features = len(features)

        # Apply feature importance weighting if enabled
        if self.feature_importance_enabled and self.feature_weights is not None:
            features = features * self.feature_weights[:len(features)]

        # Apply data-adaptive preprocessing if enabled
        if self.optimization_enabled and encoding_params:
            features = self._apply_adaptive_preprocessing(features, encoding_params)

        # Route to appropriate encoding method
        encoding_methods = {
            "angle": self._angle_encoding,
            "amplitude": self._amplitude_encoding,
            "iqp": self._iqp_encoding,
            "basis": self._basis_encoding,
            "displacement": self._displacement_encoding,
            "squeezing": self._squeezing_encoding,
            "data_reuploading": self._data_reuploading_encoding,
            "feature_map": self._feature_map_encoding
        }

        if self.encoding_type not in encoding_methods:
            raise ValueError(f"Unknown encoding type: {self.encoding_type}")

        encoding_methods[self.encoding_type](features, n_features)

    def _apply_adaptive_preprocessing(self, features: np.ndarray,
                                    encoding_params: Dict) -> np.ndarray:
        """Apply data-adaptive preprocessing to features"""
        # Normalization strategy
        if 'normalization' in encoding_params:
            if encoding_params['normalization'] == 'standard':
                features = (features - np.mean(features)) / (np.std(features) + 1e-8)
            elif encoding_params['normalization'] == 'minmax':
                features = (features - np.min(features)) / (np.max(features) - np.min(features) + 1e-8)

        # Feature scaling
        if 'scaling_factors' in encoding_params:
            features = features * encoding_params['scaling_factors'][:len(features)]

        return features

    def _angle_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Angle encoding: RY rotations with feature values as angles
        Simple and hardware-efficient encoding scheme
        """
        for i in range(min(n_features, self.n_qubits)):
            qml.RY(features[i], wires=i)

    def _amplitude_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Amplitude encoding: Encode features into quantum state amplitudes
        More expressive but requires more qubits
        """
        # Pad or truncate features to fit 2^n_qubits amplitudes
        max_amplitudes = 2 ** self.n_qubits

        if n_features > max_amplitudes:
            # Truncate if too many features
            padded_features = features[:max_amplitudes]
        else:
            # Pad with zeros if too few features
            padded_features = np.pad(features, (0, max_amplitudes - n_features))

        # Normalize to valid quantum state
        norm = np.linalg.norm(padded_features)
        if norm > 0:
            normalized_features = padded_features / norm
        else:
            normalized_features = np.ones(max_amplitudes) / np.sqrt(max_amplitudes)

        qml.QubitStateVector(normalized_features, wires=range(self.n_qubits))

    def _iqp_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Instantaneous Quantum Polynomial (IQP) encoding
        Creates entanglement through ZZ interactions
        """
        # Initial RX rotations
        for i in range(min(n_features, self.n_qubits)):
            qml.RX(features[i], wires=i)

        # ZZ entangling layers
        for i in range(self.n_qubits - 1):
            for j in range(i + 1, self.n_qubits):
                if i < n_features and j < n_features:
                    qml.IsingZZ(features[i] * features[j], wires=[i, j])

    def _basis_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Basis encoding: Map classical data to computational basis states
        Suitable for discrete or categorical features
        """
        # Convert features to binary representation
        for i in range(min(n_features, self.n_qubits)):
            # Threshold-based binary encoding
            if features[i] > 0.5:
                qml.PauliX(wires=i)

    def _displacement_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Displacement encoding: Use displacement operations for continuous variables
        Particularly suitable for PK/PD parameters with continuous nature
        """
        # Apply displacement in both X and P quadratures
        for i in range(min(n_features, self.n_qubits)):
            # RX rotation for X quadrature
            qml.RX(features[i] * np.pi, wires=i)
            # RY rotation for P quadrature (phase-shifted)
            if i + 1 < len(features):
                qml.RY(features[i] * np.pi / 2, wires=i)

    def _squeezing_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Squeezing encoding: Use squeezing operations for enhanced encoding capacity
        Provides richer quantum state space exploration
        """
        for i in range(min(n_features, self.n_qubits)):
            # Squeezing parameter proportional to feature value
            r = features[i] * 0.5  # Moderate squeezing
            phi = features[i] * np.pi / 4  # Phase parameter

            # Approximate squeezing using rotation gates
            qml.RX(r * np.cos(phi), wires=i)
            qml.RY(r * np.sin(phi), wires=i)

    def _data_reuploading_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Data reuploading encoding: Multiple layers of feature encoding
        Increases model expressivity through repeated data encoding
        """
        n_reupload_layers = min(3, self.n_qubits // 2 + 1)

        for layer in range(n_reupload_layers):
            for i in range(min(n_features, self.n_qubits)):
                # Different rotation axes for each layer
                if layer % 3 == 0:
                    qml.RX(features[i], wires=i)
                elif layer % 3 == 1:
                    qml.RY(features[i], wires=i)
                else:
                    qml.RZ(features[i], wires=i)

            # Add entanglement between reuploading layers
            if layer < n_reupload_layers - 1:
                for i in range(self.n_qubits - 1):
                    qml.CNOT(wires=[i, i + 1])

    def _feature_map_encoding(self, features: np.ndarray, n_features: int) -> None:
        """
        Feature map encoding: Pauli feature map with entanglement
        Creates rich quantum feature space through Pauli operators
        """
        # First-order features
        for i in range(min(n_features, self.n_qubits)):
            qml.RZ(2 * features[i], wires=i)

        # Second-order features (entangling)
        for i in range(min(n_features - 1, self.n_qubits - 1)):
            for j in range(i + 1, min(n_features, self.n_qubits)):
                if j < self.n_qubits:
                    # ZZ interaction
                    qml.CNOT(wires=[i, j])
                    qml.RZ(2 * features[i] * features[j], wires=j)
                    qml.CNOT(wires=[i, j])

    def set_feature_weights(self, weights: np.ndarray) -> None:
        """Set feature importance weights for encoding"""
        self.feature_weights = np.array(weights)

    def analyze_encoding_performance(self, performance_metric: float) -> None:
        """Track encoding performance for optimization"""
        self.performance_history.append(performance_metric)

    def get_optimal_encoding_params(self, features_batch: np.ndarray) -> Dict:
        """
        Determine optimal encoding parameters based on data statistics

        Args:
            features_batch: Batch of features for analysis

        Returns:
            Optimal encoding parameters
        """
        # Analyze data statistics
        feature_stats = {
            'mean': np.mean(features_batch, axis=0),
            'std': np.std(features_batch, axis=0),
            'min': np.min(features_batch, axis=0),
            'max': np.max(features_batch, axis=0)
        }

        # Determine optimal normalization strategy
        cv = feature_stats['std'] / (feature_stats['mean'] + 1e-8)  # Coefficient of variation
        normalization = 'standard' if np.mean(cv) > 0.5 else 'minmax'

        # Determine scaling factors based on feature ranges
        feature_ranges = feature_stats['max'] - feature_stats['min']
        scaling_factors = 1.0 / (feature_ranges + 1e-8)

        return {
            'normalization': normalization,
            'scaling_factors': scaling_factors,
            'feature_stats': feature_stats
        }


class VariationalAnsatz:
    """Enhanced variational ansatz with Phase 2B alternative architectures"""

    def __init__(self, ansatz_type: str, n_qubits: int, n_layers: int,
                 optimization_params: Optional[Dict] = None):
        self.ansatz_type = ansatz_type
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.optimization_params = optimization_params or {}

        # Phase 2B: Performance tracking
        self.expressivity_score = None
        # Limit gradient history to prevent memory issues - keep last 200 entries
        from collections import deque
        self.gradient_magnitude_history = deque(maxlen=200)

    def apply(self, params: np.ndarray) -> None:
        """
        Apply variational ansatz with given parameters

        Args:
            params: Variational parameters (shape depends on ansatz type)
        """
        ansatz_methods = {
            "ry_cnot": self._ry_cnot_ansatz,
            "strongly_entangling": self._strongly_entangling_ansatz,
            "hardware_efficient": self._hardware_efficient_ansatz,
            "optimized_hardware_efficient": self._optimized_hardware_efficient_ansatz,
            "qaoa_inspired": self._qaoa_inspired_ansatz,
            "pkpd_specific": self._pkpd_specific_ansatz
        }

        if self.ansatz_type not in ansatz_methods:
            raise ValueError(f"Unknown ansatz type: {self.ansatz_type}")

        ansatz_methods[self.ansatz_type](params)

    def get_param_shape(self) -> Tuple[int, ...]:
        """Get the required parameter shape for this ansatz"""
        param_shapes = {
            "ry_cnot": (self.n_layers, self.n_qubits),
            "strongly_entangling": (self.n_layers, self.n_qubits, 3),
            "hardware_efficient": (self.n_layers, self.n_qubits),  # Single rotation per qubit
            "optimized_hardware_efficient": (self.n_layers * self.n_qubits * 3,),
            "qaoa_inspired": (self.n_layers * 2,),  # Beta and gamma parameters
            "pkpd_specific": (self.n_layers, self.n_qubits, 4)  # RX, RY, RZ, and interaction
        }

        if self.ansatz_type not in param_shapes:
            raise ValueError(f"Unknown ansatz type: {self.ansatz_type}")

        return param_shapes[self.ansatz_type]

    def _ry_cnot_ansatz(self, params: np.ndarray) -> None:
        """
        RY-CNOT ansatz: Alternating RY rotations and CNOT gates
        Hardware-efficient and commonly used in NISQ devices
        """
        params = params.reshape(self.n_layers, self.n_qubits)

        for layer in range(self.n_layers):
            # RY rotations
            for qubit in range(self.n_qubits):
                qml.RY(params[layer, qubit], wires=qubit)

            # CNOT entangling layer (circular connectivity)
            for qubit in range(self.n_qubits):
                qml.CNOT(wires=[qubit, (qubit + 1) % self.n_qubits])

    def _strongly_entangling_ansatz(self, params: np.ndarray) -> None:
        """
        Strongly entangling ansatz using PennyLane template
        Higher expressivity but requires more parameters
        """
        params = params.reshape(self.n_layers, self.n_qubits, 3)
        qml.StronglyEntanglingLayers(params, wires=range(self.n_qubits))

    def _hardware_efficient_ansatz(self, params: np.ndarray) -> None:
        """
        Hardware-efficient ansatz with single RY rotation per qubit
        Designed to prevent barren plateaus while maintaining expressivity
        """
        params = params.reshape(self.n_layers, self.n_qubits)

        for layer in range(self.n_layers):
            # Single RY rotation per qubit (prevents barren plateaus)
            for qubit in range(self.n_qubits):
                qml.RY(params[layer, qubit], wires=qubit)

            # Local entanglement only (nearest neighbor)
            for qubit in range(self.n_qubits - 1):
                qml.CNOT(wires=[qubit, qubit + 1])

    def _optimized_hardware_efficient_ansatz(self, params: np.ndarray) -> None:
        """
        Phase 2B: Optimized hardware-efficient ansatz with adaptive connectivity
        Enhanced version with RX, RY, RZ rotations and optimized gate ordering
        """
        param_idx = 0

        for layer in range(self.n_layers):
            # Enhanced single-qubit rotations (RX, RY, RZ)
            for qubit in range(self.n_qubits):
                qml.RX(params[param_idx], wires=qubit)
                param_idx += 1
                qml.RY(params[param_idx], wires=qubit)
                param_idx += 1
                qml.RZ(params[param_idx], wires=qubit)
                param_idx += 1

            # Adaptive connectivity pattern
            connectivity_pattern = self.optimization_params.get('connectivity', 'linear')

            if connectivity_pattern == 'all_to_all':
                # All-to-all connectivity for maximum expressivity
                for i in range(self.n_qubits):
                    for j in range(i + 1, self.n_qubits):
                        qml.CNOT(wires=[i, j])
            elif connectivity_pattern == 'circular':
                # Circular connectivity
                for qubit in range(self.n_qubits):
                    qml.CNOT(wires=[qubit, (qubit + 1) % self.n_qubits])
            else:  # Default: linear
                for qubit in range(self.n_qubits - 1):
                    qml.CNOT(wires=[qubit, qubit + 1])

    def _qaoa_inspired_ansatz(self, params: np.ndarray) -> None:
        """
        Phase 2B: QAOA-inspired ansatz for combinatorial optimization aspects
        Alternates between cost and mixer Hamiltonians
        """
        beta_params = params[:self.n_layers]  # Mixer parameters
        gamma_params = params[self.n_layers:]  # Cost parameters

        for layer in range(self.n_layers):
            # Cost Hamiltonian layer (problem-specific)
            # For PK/PD: model parameter interactions
            for i in range(self.n_qubits - 1):
                for j in range(i + 1, self.n_qubits):
                    # ZZ interactions representing parameter correlations
                    qml.CNOT(wires=[i, j])
                    qml.RZ(gamma_params[layer], wires=j)
                    qml.CNOT(wires=[i, j])

            # Mixer Hamiltonian layer (X rotations)
            for qubit in range(self.n_qubits):
                qml.RX(beta_params[layer], wires=qubit)

    def _pkpd_specific_ansatz(self, params: np.ndarray) -> None:
        """
        Phase 2B: PK/PD-specific ansatz designed for pharmacokinetic modeling
        Incorporates domain knowledge about PK/PD parameter relationships
        """
        params = params.reshape(self.n_layers, self.n_qubits, 4)

        for layer in range(self.n_layers):
            # Absorption phase modeling (RX rotations)
            for qubit in range(self.n_qubits):
                qml.RX(params[layer, qubit, 0], wires=qubit)

            # Distribution phase modeling (RY rotations)
            for qubit in range(self.n_qubits):
                qml.RY(params[layer, qubit, 1], wires=qubit)

            # Metabolism/elimination modeling (RZ rotations)
            for qubit in range(self.n_qubits):
                qml.RZ(params[layer, qubit, 2], wires=qubit)

            # Inter-compartment interactions
            if self.n_qubits >= 2:
                # Central-peripheral compartment interactions
                qml.IsingXX(params[layer, 0, 3], wires=[0, 1])

                # Additional compartment interactions for complex models
                if self.n_qubits >= 4:
                    # Tissue distribution modeling
                    qml.IsingYY(params[layer, 1, 3], wires=[1, 2])
                    qml.IsingZZ(params[layer, 2, 3], wires=[2, 3])

                # Body weight and concomitant medication effects
                if self.n_qubits > 2:
                    for i in range(2, self.n_qubits):
                        qml.CRY(params[layer, i, 3], wires=[0, i])

    def calculate_expressivity(self, n_samples: int = 1000) -> float:
        """
        Calculate expressivity score for the ansatz architecture

        Args:
            n_samples: Number of random parameter samples for evaluation

        Returns:
            Expressivity score (0-1, higher is more expressive)
        """
        # Generate random parameter samples
        param_shape = self.get_param_shape()
        random_params = [np.random.uniform(-np.pi, np.pi, param_shape)
                        for _ in range(n_samples)]

        # Create a temporary device for evaluation
        dev = qml.device('default.qubit', wires=self.n_qubits)

        @qml.qnode(dev)
        def test_circuit(params):
            self.apply(params)
            return qml.state()

        # Calculate state diversity
        states = []
        for params in random_params:
            try:
                state = test_circuit(params)
                states.append(state)
            except Exception:
                continue

        if not states:
            return 0.0

        # Calculate pairwise distances between states
        states = np.array(states)
        n_states = len(states)
        distances = []

        for i in range(n_states):
            for j in range(i + 1, n_states):
                # Fidelity-based distance
                fidelity = np.abs(np.vdot(states[i], states[j]))**2
                distance = 1 - fidelity
                distances.append(distance)

        # Expressivity is the average distance
        expressivity = np.mean(distances) if distances else 0.0
        self.expressivity_score = expressivity

        return expressivity

    def track_gradient_magnitudes(self, gradients: np.ndarray) -> None:
        """Track gradient magnitudes for barren plateau detection"""
        grad_magnitude = np.linalg.norm(gradients)
        self.gradient_magnitude_history.append(grad_magnitude)

    def detect_barren_plateau(self, threshold: float = 1e-6,
                            window_size: int = 10) -> bool:
        """
        Detect if the ansatz is in a barren plateau

        Args:
            threshold: Gradient magnitude threshold
            window_size: Number of recent iterations to consider

        Returns:
            True if barren plateau detected
        """
        if len(self.gradient_magnitude_history) < window_size:
            return False

        recent_gradients = self.gradient_magnitude_history[-window_size:]
        average_gradient = np.mean(recent_gradients)

        return average_gradient < threshold


class AdaptiveDepthCircuit:
    """
    Phase 2B: Adaptive circuit depth management for dynamic optimization

    This class manages circuit depth adaptation during training to:
    - Avoid barren plateaus through progressive depth increase
    - Enable layer-wise training for better gradient flow
    - Optimize depth vs accuracy trade-offs
    """

    def __init__(self, config: CircuitConfig):
        self.config = config
        self.current_depth = config.min_layers
        # Limit histories to prevent memory issues
        from collections import deque
        self.training_history = deque(maxlen=200)
        self.depth_adjustment_history = deque(maxlen=100)
        self.layer_wise_progress = {}

    def should_increase_depth(self, gradient_magnitude: float,
                            loss_improvement: float,
                            patience: int = 5) -> bool:
        """
        Determine if circuit depth should be increased

        Args:
            gradient_magnitude: Current gradient magnitude
            loss_improvement: Recent loss improvement
            patience: Number of iterations to wait before adjustment

        Returns:
            True if depth should be increased
        """
        # Check if in barren plateau
        if gradient_magnitude < self.config.depth_adjustment_threshold:
            return True

        # Check if loss improvement has stagnated
        if len(self.training_history) >= patience:
            recent_improvements = [h['loss_improvement']
                                 for h in self.training_history[-patience:]]
            avg_improvement = np.mean(recent_improvements)

            if avg_improvement < 0.01:  # Threshold for stagnation
                return True

        return False

    def adjust_depth(self, increase: bool = True) -> int:
        """
        Adjust circuit depth adaptively

        Args:
            increase: Whether to increase (True) or decrease (False) depth

        Returns:
            New circuit depth
        """
        if increase and self.current_depth < self.config.max_layers:
            self.current_depth += 1
            adjustment_type = "increase"
        elif not increase and self.current_depth > self.config.min_layers:
            self.current_depth -= 1
            adjustment_type = "decrease"
        else:
            adjustment_type = "no_change"

        self.depth_adjustment_history.append({
            'iteration': len(self.training_history),
            'new_depth': self.current_depth,
            'adjustment': adjustment_type
        })

        return self.current_depth

    def initialize_layer_wise_training(self, base_params: np.ndarray) -> np.ndarray:
        """
        Initialize parameters for layer-wise training

        Args:
            base_params: Base parameter array

        Returns:
            Extended parameter array for current depth
        """
        if not self.config.layer_wise_training:
            return base_params

        # Create ansatz to get proper parameter shape
        ansatz = VariationalAnsatz(
            self.config.ansatz,
            self.config.n_qubits,
            self.current_depth
        )

        target_shape = ansatz.get_param_shape()

        # Initialize additional parameters for new layers
        if len(target_shape) == 1:
            # Flattened parameters (e.g., hardware_efficient)
            new_params = np.random.normal(0, 0.1, target_shape)
            if len(base_params) > 0:
                copy_size = min(len(base_params), len(new_params))
                new_params[:copy_size] = base_params[:copy_size]
        elif len(target_shape) == 2:
            # Layer x qubit structure
            new_params = np.random.normal(0, 0.1, target_shape)
            if base_params.size > 0:
                base_layers = min(base_params.shape[0], target_shape[0])
                new_params[:base_layers, :] = base_params[:base_layers, :]
        else:
            # More complex structures
            new_params = np.random.normal(0, 0.1, target_shape)
            # Copy what we can from base parameters
            if base_params.size > 0:
                flat_base = base_params.flatten()
                flat_new = new_params.flatten()
                copy_size = min(len(flat_base), len(flat_new))
                flat_new[:copy_size] = flat_base[:copy_size]
                new_params = flat_new.reshape(target_shape)

        return new_params

    def record_training_step(self, loss: float, gradient_magnitude: float,
                           loss_improvement: float) -> None:
        """Record training step for depth adaptation decisions"""
        self.training_history.append({
            'iteration': len(self.training_history),
            'depth': self.current_depth,
            'loss': loss,
            'gradient_magnitude': gradient_magnitude,
            'loss_improvement': loss_improvement
        })

        # Update layer-wise progress tracking
        if self.config.layer_wise_training:
            layer_key = f"depth_{self.current_depth}"
            if layer_key not in self.layer_wise_progress:
                self.layer_wise_progress[layer_key] = {
                    'start_iteration': len(self.training_history),
                    'losses': [],
                    'converged': False
                }

            self.layer_wise_progress[layer_key]['losses'].append(loss)

    def check_layer_convergence(self, convergence_window: int = 10,
                              convergence_threshold: float = 1e-4) -> bool:
        """
        Check if current layer has converged for layer-wise training

        Args:
            convergence_window: Number of recent iterations to check
            convergence_threshold: Threshold for loss improvement

        Returns:
            True if layer has converged
        """
        if not self.config.layer_wise_training:
            return False

        layer_key = f"depth_{self.current_depth}"
        if layer_key not in self.layer_wise_progress:
            return False

        losses = self.layer_wise_progress[layer_key]['losses']
        if len(losses) < convergence_window:
            return False

        # Check loss improvement in recent window
        recent_losses = losses[-convergence_window:]
        loss_improvements = [recent_losses[i-1] - recent_losses[i]
                           for i in range(1, len(recent_losses))]

        avg_improvement = np.mean(loss_improvements)

        converged = avg_improvement < convergence_threshold
        if converged:
            self.layer_wise_progress[layer_key]['converged'] = True

        return converged

    def get_training_summary(self) -> Dict:
        """Get summary of adaptive depth training"""
        return {
            'current_depth': self.current_depth,
            'depth_range': (self.config.min_layers, self.config.max_layers),
            'total_adjustments': len(self.depth_adjustment_history),
            'training_iterations': len(self.training_history),
            'layer_wise_enabled': self.config.layer_wise_training,
            'layer_progress': self.layer_wise_progress,
            'adjustment_history': self.depth_adjustment_history[-10:]  # Last 10 adjustments
        }


class VQCircuit:
    """
    Enhanced VQ Circuit with Phase 2B features for PK/PD parameter estimation

    This class combines data encoding, variational ansatz, and measurement
    with adaptive depth, enhanced encodings, and benchmarking capabilities.
    """

    def __init__(self, config: CircuitConfig):
        self.config = config
        self.device = qml.device(config.device_name, wires=config.n_qubits, shots=config.shots)

        # Initialize components with Phase 2B enhancements
        self.encoder = QuantumDataEncoder(
            config.encoding,
            config.n_qubits,
            optimization_enabled=config.encoding_optimization,
            feature_importance_enabled=config.feature_importance_analysis
        )

        # Initialize ansatz with current depth (adaptive if enabled)
        current_layers = config.min_layers if config.adaptive_depth else config.n_layers
        self.ansatz = VariationalAnsatz(config.ansatz, config.n_qubits, current_layers)

        # Phase 2B: Adaptive depth management
        self.adaptive_depth_manager = None
        if config.adaptive_depth:
            self.adaptive_depth_manager = AdaptiveDepthCircuit(config)

        # Create quantum node
        self.qnode = qml.QNode(self._circuit, self.device, diff_method=config.diff_method)

        # Parameter shape for initialization
        self.param_shape = self.ansatz.get_param_shape()

        # Phase 2B: Benchmarking and analysis
        self.performance_metrics = {}
        self.encoding_comparison_results = {}
        self.expressivity_scores = {}

    def _circuit(self, params: np.ndarray, features: np.ndarray,
                encoding_params: Optional[Dict] = None) -> List[float]:
        """
        Complete quantum circuit: encoding + ansatz + measurement

        Args:
            params: Variational parameters
            features: Classical input features
            encoding_params: Optional encoding parameters for adaptive encoding

        Returns:
            List of expectation values for each qubit
        """
        # Enhanced data encoding with Phase 2B features
        self.encoder.encode(features, encoding_params)

        # Variational ansatz
        self.ansatz.apply(params)

        # Measurements (Pauli-Z expectation values)
        return [qml.expval(qml.PauliZ(i)) for i in range(self.config.n_qubits)]

    def forward(self, params: np.ndarray, features: np.ndarray,
              encoding_params: Optional[Dict] = None) -> np.ndarray:
        """
        Enhanced forward pass with Phase 2B features

        Args:
            params: Variational parameters
            features: Input features [time, dose, body_weight, concomitant_med]
            encoding_params: Optional encoding parameters for adaptive encoding

        Returns:
            Quantum circuit outputs (expectation values)
        """
        # Update quantum node if adaptive depth is enabled
        if self.adaptive_depth_manager:
            current_depth = self.adaptive_depth_manager.current_depth
            if current_depth != self.ansatz.n_layers:
                self._update_circuit_depth(current_depth)

        # Create a wrapper for the circuit with encoding parameters
        @qml.qnode(self.device, diff_method=self.config.diff_method)
        def enhanced_circuit(params, features):
            return self._circuit(params, features, encoding_params)

        outputs = enhanced_circuit(params, features)
        return np.array(outputs)

    def _update_circuit_depth(self, new_depth: int) -> None:
        """Update circuit depth and reinitialize ansatz"""
        self.ansatz = VariationalAnsatz(
            self.config.ansatz,
            self.config.n_qubits,
            new_depth
        )
        self.param_shape = self.ansatz.get_param_shape()

    def adaptive_training_step(self, params: np.ndarray, features: np.ndarray,
                             loss: float, gradients: np.ndarray) -> Tuple[np.ndarray, bool]:
        """
        Perform adaptive training step with depth adjustment

        Args:
            params: Current parameters
            features: Training features
            loss: Current loss value
            gradients: Current gradients

        Returns:
            Tuple of (updated_parameters, depth_changed)
        """
        if not self.adaptive_depth_manager:
            return params, False

        # Calculate gradient magnitude and loss improvement
        gradient_magnitude = np.linalg.norm(gradients)
        loss_improvement = 0.0
        if len(self.adaptive_depth_manager.training_history) > 0:
            prev_loss = self.adaptive_depth_manager.training_history[-1]['loss']
            loss_improvement = prev_loss - loss

        # Record training step
        self.adaptive_depth_manager.record_training_step(
            loss, gradient_magnitude, loss_improvement
        )

        # Track gradients for barren plateau detection
        self.ansatz.track_gradient_magnitudes(gradients)

        # Check if depth should be adjusted
        depth_changed = False
        if self.adaptive_depth_manager.should_increase_depth(gradient_magnitude, loss_improvement):
            new_depth = self.adaptive_depth_manager.adjust_depth(increase=True)
            new_params = self.adaptive_depth_manager.initialize_layer_wise_training(params)
            self._update_circuit_depth(new_depth)
            depth_changed = True
            return new_params, depth_changed

        # Check for layer-wise convergence
        if self.config.layer_wise_training:
            if self.adaptive_depth_manager.check_layer_convergence():
                new_depth = self.adaptive_depth_manager.adjust_depth(increase=True)
                if new_depth != self.adaptive_depth_manager.current_depth - 1:
                    new_params = self.adaptive_depth_manager.initialize_layer_wise_training(params)
                    self._update_circuit_depth(new_depth)
                    depth_changed = True
                    return new_params, depth_changed

        return params, depth_changed

    def initialize_parameters(self, seed: Optional[int] = None,
                             strategy: str = "layer_wise") -> np.ndarray:
        """
        Initialize variational parameters with barren plateau mitigation and PennyLane array configuration

        Uses research-backed strategies to prevent barren plateaus in VQCs:
        - Layer-wise variance scaling for deep circuits
        - Identity-biased initialization for better gradient flow
        - Ansatz-specific parameter scaling
        - Proper PennyLane array configuration for gradient computation

        Args:
            seed: Random seed for reproducibility
            strategy: Initialization strategy ("layer_wise", "identity_biased", "xavier_quantum")

        Returns:
            Initialized parameter array properly configured for PennyLane gradient computation

        Raises:
            ValueError: If parameter initialization fails or returns empty array
        """
        if seed is not None:
            np.random.seed(seed)

        # Initialize parameters based on strategy
        if strategy == "layer_wise":
            params = self._layer_wise_initialization()
        elif strategy == "identity_biased":
            params = self._identity_biased_initialization()
        elif strategy == "xavier_quantum":
            return self._xavier_quantum_initialization()
        else:
            raise ValueError(f"Unknown initialization strategy: {strategy}")

        # Post-initialization validation and PennyLane configuration
        if params.size == 0:
            raise ValueError(f"Parameter initialization returned empty array for strategy '{strategy}'")

        # Ensure parameters are properly shaped numpy array
        if not isinstance(params, np.ndarray):
            params = np.array(params, dtype=np.float64)
        else:
            params = params.astype(np.float64)

        # Validate parameter shape matches expected circuit shape
        if params.shape != self.param_shape:
            raise ValueError(f"Initialized parameter shape {params.shape} doesn't match expected {self.param_shape}")

        # Check for invalid values (NaN, Inf)
        if not np.isfinite(params).all():
            raise ValueError("Parameter initialization produced non-finite values")

        # For PennyLane gradient computation, convert to qml.numpy and mark as trainable
        params = qml.numpy.array(params, requires_grad=True)
        return params

    def _layer_wise_initialization(self) -> np.ndarray:
        """
        Layer-wise variance scaling initialization

        Addresses barren plateaus by scaling parameter variance based on:
        - Circuit depth (layer position)
        - Number of qubits (width effects)
        - Ansatz type (expressivity requirements)

        Reference: https://doi.org/10.1038/s41467-021-21728-w
        """
        params = np.zeros(self.param_shape)

        if self.config.ansatz == "ry_cnot":
            # Shape: (n_layers, n_qubits)
            for layer in range(self.config.n_layers):
                # Decay variance with depth to maintain gradient flow
                layer_factor = 1.0 / np.sqrt(layer + 1)

                # Scale by number of qubits to prevent exponential scaling
                width_factor = 1.0 / np.sqrt(self.config.n_qubits)

                # Base variance for RY gates (empirically determined)
                base_variance = 0.8

                layer_std = base_variance * layer_factor * width_factor

                # Initialize with small random values around zero
                params[layer, :] = np.random.normal(0, layer_std, self.config.n_qubits)

        elif self.config.ansatz == "strongly_entangling":
            # Shape: (n_layers, n_qubits, 3) - RX, RY, RZ rotations
            for layer in range(self.config.n_layers):
                layer_factor = 1.0 / np.sqrt(layer + 1)
                width_factor = 1.0 / np.sqrt(self.config.n_qubits)
                base_variance = 0.6  # Lower for more complex ansatz

                layer_std = base_variance * layer_factor * width_factor

                params[layer, :, :] = np.random.normal(0, layer_std,
                                                     (self.config.n_qubits, 3))

        elif self.config.ansatz == "hardware_efficient":
            # Shape: (n_layers, n_qubits) - single RY rotation per qubit
            for layer in range(self.config.n_layers):
                # Stronger variance decay for barren plateau prevention
                layer_factor = 1.0 / np.sqrt(layer + 1)
                width_factor = 1.0 / np.sqrt(self.config.n_qubits)
                # Much smaller base variance to prevent barren plateaus
                base_variance = 0.01  # Reduced from 0.7

                layer_std = base_variance * layer_factor * width_factor

                # Single RY rotation per qubit (prevents barren plateaus)
                params[layer, :] = np.random.normal(0, layer_std, self.config.n_qubits)

        return params

    def _identity_biased_initialization(self) -> np.ndarray:
        """
        Identity-biased initialization for stable training

        Initializes parameters close to identity operations to:
        - Start optimization in a well-behaved region
        - Ensure initial gradients are meaningful
        - Provide good conditioning for the optimization landscape
        """
        # Small random perturbations around identity (zero rotations)
        identity_std = 0.1  # Very small perturbations

        # Add structured noise to break symmetry
        params = np.random.normal(0, identity_std, self.param_shape)

        # For deeper circuits, add slight bias toward identity in later layers
        if self.config.ansatz == "ry_cnot" and len(self.param_shape) >= 2:
            for layer in range(self.config.n_layers):
                # Later layers get smaller perturbations
                decay_factor = 0.8 ** layer
                params[layer, :] *= decay_factor

        elif self.config.ansatz == "strongly_entangling" and len(self.param_shape) >= 3:
            for layer in range(self.config.n_layers):
                decay_factor = 0.8 ** layer
                params[layer, :, :] *= decay_factor

        return params

    def _xavier_quantum_initialization(self) -> np.ndarray:
        """
        Quantum-adapted Xavier initialization

        Maintains the Xavier principle but with quantum-specific modifications:
        - Accounts for periodic nature of quantum gates
        - Considers measurement statistics
        - Balances expressivity with trainability
        """
        # Calculate effective fan-in and fan-out for quantum case
        if len(self.param_shape) == 1:
            # Flattened parameters
            fan_in = fan_out = int(np.sqrt(self.param_shape[0]))
        elif len(self.param_shape) == 2:
            # Layer x qubit structure
            fan_in = self.param_shape[1]  # qubits
            fan_out = self.param_shape[0]  # layers
        else:
            # More complex structure (e.g., strongly entangling)
            fan_in = np.prod(self.param_shape[1:])
            fan_out = self.param_shape[0]

        # Quantum-adapted Xavier limit
        # Factor of Ï€/2 accounts for quantum gate periodicity
        # Factor of 2 accounts for expectation value range [-1, 1]
        xavier_limit = np.sqrt(6.0 / (fan_in + fan_out))
        quantum_limit = xavier_limit * (np.pi / 4)  # Conservative quantum scaling

        # Uniform distribution for better exploration in periodic space
        return np.random.uniform(-quantum_limit, quantum_limit, self.param_shape)

    def compare_encoding_strategies(self, features_batch: np.ndarray,
                                  target_outputs: Optional[np.ndarray] = None) -> Dict:
        """
        Phase 2B: Compare different encoding strategies on given data

        Args:
            features_batch: Batch of features for comparison
            target_outputs: Optional target outputs for supervised comparison

        Returns:
            Comparison results for different encoding strategies
        """
        if not self.config.encoding_comparison:
            return {}

        encoding_strategies = [
            "angle", "amplitude", "iqp", "basis", "displacement",
            "squeezing", "data_reuploading", "feature_map"
        ]

        comparison_results = {}
        original_encoding = self.config.encoding

        # Initialize parameters once
        params = self.initialize_parameters()

        for encoding in encoding_strategies:
            try:
                # Temporarily change encoding
                self.encoder.encoding_type = encoding

                # Test circuit performance
                outputs = []
                for features in features_batch:
                    try:
                        output = self.forward(params, features)
                        outputs.append(output)
                    except Exception as e:
                        print(f"Warning: Encoding {encoding} failed on sample: {e}")
                        break

                if outputs:
                    outputs = np.array(outputs)

                    # Calculate encoding metrics
                    output_variance = np.var(outputs, axis=0).mean()
                    output_range = np.ptp(outputs, axis=0).mean()
                    output_mean = np.mean(outputs, axis=0)

                    # Expressivity measure
                    expressivity = self._calculate_encoding_expressivity(outputs)

                    comparison_results[encoding] = {
                        'expressivity': expressivity,
                        'output_variance': output_variance,
                        'output_range': output_range,
                        'mean_output': output_mean.tolist(),
                        'num_successful_samples': len(outputs)
                    }

                    # If target outputs provided, calculate accuracy
                    if target_outputs is not None and len(outputs) == len(target_outputs):
                        mse = np.mean((outputs - target_outputs[:len(outputs)])**2)
                        comparison_results[encoding]['mse'] = mse

            except Exception as e:
                comparison_results[encoding] = {'error': str(e)}

        # Restore original encoding
        self.encoder.encoding_type = original_encoding
        self.encoding_comparison_results = comparison_results

        return comparison_results

    def _calculate_encoding_expressivity(self, outputs: np.ndarray) -> float:
        """Calculate expressivity measure for encoding strategy"""
        if len(outputs) < 2:
            return 0.0

        # Calculate pairwise distances between outputs
        distances = []
        for i in range(len(outputs)):
            for j in range(i + 1, len(outputs)):
                distance = np.linalg.norm(outputs[i] - outputs[j])
                distances.append(distance)

        return np.mean(distances) if distances else 0.0

    def benchmark_ansatz_expressivity(self, n_samples: int = 500) -> Dict:
        """
        Phase 2B: Comprehensive expressivity benchmarking for all ansatz types

        Args:
            n_samples: Number of random samples for benchmarking

        Returns:
            Expressivity scores for different ansatz types
        """
        if not self.config.expressivity_benchmarking:
            return {}

        ansatz_types = [
            "ry_cnot", "strongly_entangling", "hardware_efficient",
            "optimized_hardware_efficient", "qaoa_inspired", "pkpd_specific"
        ]

        original_ansatz = self.config.ansatz
        original_layers = self.ansatz.n_layers
        expressivity_results = {}

        for ansatz_type in ansatz_types:
            try:
                # Create temporary ansatz for testing
                test_ansatz = VariationalAnsatz(
                    ansatz_type,
                    self.config.n_qubits,
                    original_layers
                )

                expressivity = test_ansatz.calculate_expressivity(n_samples)
                expressivity_results[ansatz_type] = {
                    'expressivity_score': expressivity,
                    'parameter_count': np.prod(test_ansatz.get_param_shape()),
                    'expressivity_per_param': expressivity / max(1, np.prod(test_ansatz.get_param_shape()))
                }

            except Exception as e:
                expressivity_results[ansatz_type] = {'error': str(e)}

        self.expressivity_scores = expressivity_results
        return expressivity_results

    def analyze_feature_importance(self, features_batch: np.ndarray,
                                 target_outputs: np.ndarray) -> Dict:
        """
        Phase 2B: Analyze feature importance for PK/PD modeling

        Args:
            features_batch: Batch of features
            target_outputs: Target outputs for supervised analysis

        Returns:
            Feature importance analysis results
        """
        if not self.config.feature_importance_analysis:
            return {}

        n_features = features_batch.shape[1]
        params = self.initialize_parameters()

        # Baseline performance
        baseline_outputs = []
        for features in features_batch:
            output = self.forward(params, features)
            baseline_outputs.append(output)
        baseline_outputs = np.array(baseline_outputs)

        baseline_mse = np.mean((baseline_outputs - target_outputs)**2)

        # Feature importance through perturbation
        feature_importance = {}

        for feature_idx in range(n_features):
            # Create perturbed feature batch
            perturbed_batch = features_batch.copy()
            # Add noise to this feature
            noise_std = np.std(perturbed_batch[:, feature_idx]) * 0.1
            perturbed_batch[:, feature_idx] += np.random.normal(0, noise_std, len(perturbed_batch))

            # Calculate performance with perturbed feature
            perturbed_outputs = []
            for features in perturbed_batch:
                output = self.forward(params, features)
                perturbed_outputs.append(output)
            perturbed_outputs = np.array(perturbed_outputs)

            perturbed_mse = np.mean((perturbed_outputs - target_outputs)**2)

            # Importance is the change in performance
            importance = (perturbed_mse - baseline_mse) / baseline_mse

            feature_names = ['time', 'dose', 'body_weight', 'concomitant_med']
            feature_name = feature_names[feature_idx] if feature_idx < len(feature_names) else f'feature_{feature_idx}'

            feature_importance[feature_name] = {
                'importance_score': importance,
                'baseline_mse': baseline_mse,
                'perturbed_mse': perturbed_mse
            }

        return feature_importance

    def optimize_encoding_parameters(self, features_batch: np.ndarray) -> Dict:
        """
        Phase 2B: Optimize encoding parameters based on data characteristics

        Args:
            features_batch: Batch of features for optimization

        Returns:
            Optimized encoding parameters
        """
        if not self.config.encoding_optimization:
            return {}

        optimal_params = self.encoder.get_optimal_encoding_params(features_batch)

        # Test different scaling strategies
        scaling_strategies = [0.5, 1.0, 1.5, 2.0]
        best_strategy = 1.0
        best_score = 0.0

        params = self.initialize_parameters()

        for scale in scaling_strategies:
            test_params = optimal_params.copy()
            if 'scaling_factors' in test_params:
                test_params['scaling_factors'] *= scale

            # Test expressivity with this scaling
            outputs = []
            for features in features_batch[:min(50, len(features_batch))]:  # Sample subset
                try:
                    output = self.forward(params, features, test_params)
                    outputs.append(output)
                except Exception:
                    continue

            if outputs:
                expressivity = self._calculate_encoding_expressivity(np.array(outputs))
                if expressivity > best_score:
                    best_score = expressivity
                    best_strategy = scale

        # Update optimal parameters with best scaling
        if 'scaling_factors' in optimal_params:
            optimal_params['scaling_factors'] *= best_strategy

        optimal_params['optimization_score'] = best_score

        return optimal_params

    def performance_analysis(self, features_batch: np.ndarray,
                           target_outputs: Optional[np.ndarray] = None) -> Dict:
        """
        Phase 2B: Comprehensive performance analysis

        Args:
            features_batch: Batch of features
            target_outputs: Optional target outputs

        Returns:
            Performance analysis results
        """
        if not self.config.performance_analysis:
            return {}

        params = self.initialize_parameters()

        # Circuit complexity analysis
        circuit_info = self.get_circuit_info()

        # Performance timing
        import time
        start_time = time.time()

        outputs = []
        for features in features_batch[:100]:  # Sample for timing
            output = self.forward(params, features)
            outputs.append(output)

        execution_time = time.time() - start_time
        avg_time_per_sample = execution_time / len(outputs)

        outputs = np.array(outputs)

        # Output analysis
        output_stats = {
            'mean': np.mean(outputs, axis=0).tolist(),
            'std': np.std(outputs, axis=0).tolist(),
            'range': np.ptp(outputs, axis=0).tolist(),
            'output_diversity': self._calculate_encoding_expressivity(outputs)
        }

        # Gradient analysis if possible
        gradient_info = {}
        if len(self.ansatz.gradient_magnitude_history) > 0:
            gradient_info = {
                'mean_gradient_magnitude': np.mean(self.ansatz.gradient_magnitude_history),
                'gradient_trend': 'decreasing' if len(self.ansatz.gradient_magnitude_history) > 1 and
                                  self.ansatz.gradient_magnitude_history[-1] < self.ansatz.gradient_magnitude_history[0]
                                  else 'stable',
                'barren_plateau_detected': self.ansatz.detect_barren_plateau()
            }

        analysis_results = {
            'circuit_complexity': circuit_info,
            'execution_time': {
                'total_time_seconds': execution_time,
                'avg_time_per_sample': avg_time_per_sample,
                'samples_per_second': 1.0 / avg_time_per_sample
            },
            'output_statistics': output_stats,
            'gradient_analysis': gradient_info
        }

        # Add accuracy metrics if targets provided
        if target_outputs is not None:
            target_subset = target_outputs[:len(outputs)]
            mse = np.mean((outputs - target_subset)**2)
            mae = np.mean(np.abs(outputs - target_subset))

            analysis_results['accuracy_metrics'] = {
                'mse': mse,
                'mae': mae,
                'r2_score': 1 - (mse / np.var(target_subset)) if np.var(target_subset) > 0 else 0
            }

        self.performance_metrics = analysis_results
        return analysis_results

    def get_circuit_info(self) -> Dict:
        """Get detailed information about the circuit structure with Phase 2B enhancements"""
        info = {
            "n_qubits": self.config.n_qubits,
            "n_layers": self.config.n_layers,
            "ansatz": self.config.ansatz,
            "encoding": self.config.encoding,
            "param_shape": self.param_shape,
            "total_params": np.prod(self.param_shape),
            "device": str(self.device),
            "diff_method": self.config.diff_method
        }

        # Phase 2B: Add adaptive depth information
        if self.config.adaptive_depth:
            info.update({
                "adaptive_depth": True,
                "min_layers": self.config.min_layers,
                "max_layers": self.config.max_layers,
                "current_depth": self.adaptive_depth_manager.current_depth if self.adaptive_depth_manager else self.config.n_layers,
                "layer_wise_training": self.config.layer_wise_training
            })

        # Phase 2B: Add encoding enhancements
        info.update({
            "encoding_optimization": self.config.encoding_optimization,
            "feature_importance_analysis": self.config.feature_importance_analysis,
            "encoding_comparison": self.config.encoding_comparison
        })

        # Phase 2B: Add benchmarking capabilities
        info.update({
            "expressivity_benchmarking": self.config.expressivity_benchmarking,
            "performance_analysis": self.config.performance_analysis
        })

        return info

    def visualize_circuit(self, params: Optional[np.ndarray] = None,
                         features: Optional[np.ndarray] = None) -> str:
        """
        Generate ASCII visualization of the quantum circuit

        Args:
            params: Example parameters (if None, uses random)
            features: Example features (if None, uses defaults)

        Returns:
            Circuit diagram as string
        """
        if params is None:
            params = self.initialize_parameters()
        if features is None:
            features = np.array([24.0, 5.0, 70.0, 0.0])  # Example: 24h, 5mg, 70kg, no comed

        # Create a drawer for the circuit
        drawer = qml.draw(self.qnode, expansion_strategy="device")
        return drawer(params, features)


# Utility functions for circuit analysis

def estimate_circuit_resources(config: CircuitConfig) -> Dict[str, int]:
    """
    Enhanced resource estimation with Phase 2B ansatz support

    Args:
        config: Circuit configuration

    Returns:
        Dictionary with resource estimates
    """
    ansatz = VariationalAnsatz(config.ansatz, config.n_qubits, config.n_layers)
    param_shape = ansatz.get_param_shape()

    # Estimate gate counts for all ansatz types
    gate_estimates = {
        "ry_cnot": {
            "single_qubit": config.n_layers * config.n_qubits,  # RY gates
            "two_qubit": config.n_layers * config.n_qubits,     # CNOT gates
            "depth": config.n_layers * 2
        },
        "strongly_entangling": {
            "single_qubit": config.n_layers * config.n_qubits * 3,  # RX, RY, RZ
            "two_qubit": config.n_layers * config.n_qubits,         # CNOT gates
            "depth": config.n_layers * 4
        },
        "hardware_efficient": {
            "single_qubit": config.n_layers * config.n_qubits * 2,  # RY, RZ
            "two_qubit": (config.n_layers - 1) * (config.n_qubits - 1),  # CNOT gates
            "depth": config.n_layers * 3
        },
        "optimized_hardware_efficient": {
            "single_qubit": config.n_layers * config.n_qubits * 3,  # RX, RY, RZ
            "two_qubit": config.n_layers * config.n_qubits,         # Adaptive connectivity
            "depth": config.n_layers * 4
        },
        "qaoa_inspired": {
            "single_qubit": config.n_layers * config.n_qubits,      # RX mixers
            "two_qubit": config.n_layers * (config.n_qubits * (config.n_qubits - 1) // 2),  # ZZ interactions
            "depth": config.n_layers * 3
        },
        "pkpd_specific": {
            "single_qubit": config.n_layers * config.n_qubits * 3,  # RX, RY, RZ
            "two_qubit": config.n_layers * config.n_qubits,         # IsingXX, IsingYY, IsingZZ, CRY
            "depth": config.n_layers * 5
        }
    }

    if config.ansatz in gate_estimates:
        estimates = gate_estimates[config.ansatz]
    else:
        # Fallback estimate
        estimates = {
            "single_qubit": config.n_layers * config.n_qubits * 2,
            "two_qubit": config.n_layers * config.n_qubits,
            "depth": config.n_layers * 3
        }

    return {
        "total_parameters": int(np.prod(param_shape)),
        "single_qubit_gates": estimates["single_qubit"],
        "two_qubit_gates": estimates["two_qubit"],
        "estimated_depth": estimates["depth"],
        "measurements": config.n_qubits,
        "adaptive_depth_enabled": config.adaptive_depth,
        "depth_range": (config.min_layers, config.max_layers) if config.adaptive_depth else (config.n_layers, config.n_layers)
    }


def compare_ansatz_complexity(n_qubits: int, n_layers: int) -> Dict[str, Dict]:
    """
    Enhanced complexity comparison with Phase 2B ansatz types

    Args:
        n_qubits: Number of qubits
        n_layers: Number of layers

    Returns:
        Comprehensive comparison dictionary
    """
    ansatz_types = [
        "ry_cnot", "strongly_entangling", "hardware_efficient",
        "optimized_hardware_efficient", "qaoa_inspired", "pkpd_specific"
    ]
    comparison = {}

    for ansatz_type in ansatz_types:
        try:
            config = CircuitConfig(n_qubits=n_qubits, n_layers=n_layers, ansatz=ansatz_type)
            resources = estimate_circuit_resources(config)

            # Enhanced expressivity estimates
            expressivity_ratings = {
                "ry_cnot": {"rating": "Medium", "score": 0.6},
                "strongly_entangling": {"rating": "High", "score": 0.8},
                "hardware_efficient": {"rating": "Medium-High", "score": 0.7},
                "optimized_hardware_efficient": {"rating": "High", "score": 0.85},
                "qaoa_inspired": {"rating": "Medium-High", "score": 0.75},
                "pkpd_specific": {"rating": "Very High", "score": 0.9}
            }

            # Hardware friendliness assessment
            hardware_ratings = {
                "ry_cnot": "Very High",
                "strongly_entangling": "Medium",
                "hardware_efficient": "High",
                "optimized_hardware_efficient": "High",
                "qaoa_inspired": "Medium-High",
                "pkpd_specific": "Medium"
            }

            # Problem suitability for PK/PD modeling
            pkpd_suitability = {
                "ry_cnot": "Medium",
                "strongly_entangling": "Medium-High",
                "hardware_efficient": "Medium-High",
                "optimized_hardware_efficient": "High",
                "qaoa_inspired": "Medium-High",
                "pkpd_specific": "Very High"
            }

            comparison[ansatz_type] = {
                **resources,
                "expressivity": expressivity_ratings[ansatz_type]["rating"],
                "expressivity_score": expressivity_ratings[ansatz_type]["score"],
                "hardware_friendliness": hardware_ratings[ansatz_type],
                "pkpd_suitability": pkpd_suitability[ansatz_type],
                "parameter_efficiency": resources["total_parameters"] / (n_qubits * n_layers),
                "gate_efficiency": (resources["single_qubit_gates"] + resources["two_qubit_gates"]) / resources["total_parameters"]
            }

        except Exception as e:
            comparison[ansatz_type] = {"error": str(e)}

    return comparison


# Phase 2B: New utility functions for enhanced analysis

def analyze_encoding_strategies(n_qubits: int, feature_samples: np.ndarray) -> Dict:
    """
    Analyze different encoding strategies for given feature samples

    Args:
        n_qubits: Number of qubits
        feature_samples: Sample features for analysis

    Returns:
        Analysis results for different encoding strategies
    """
    encoding_types = [
        "angle", "amplitude", "iqp", "basis",
        "displacement", "squeezing", "data_reuploading", "feature_map"
    ]

    analysis_results = {}

    for encoding in encoding_types:
        try:
            encoder = QuantumDataEncoder(encoding, n_qubits)

            # Test encoding capacity
            max_features = min(len(feature_samples[0]), n_qubits)

            analysis_results[encoding] = {
                "max_features_supported": max_features,
                "encoding_capacity": max_features / n_qubits,
                "suitable_for_continuous": encoding in ["angle", "displacement", "squeezing", "data_reuploading"],
                "suitable_for_discrete": encoding in ["basis", "amplitude"],
                "expressivity_potential": {
                    "angle": 0.6,
                    "amplitude": 0.8,
                    "iqp": 0.9,
                    "basis": 0.3,
                    "displacement": 0.7,
                    "squeezing": 0.8,
                    "data_reuploading": 0.9,
                    "feature_map": 0.95
                }[encoding],
                "hardware_requirements": {
                    "angle": "Basic",
                    "amplitude": "State preparation",
                    "iqp": "ZZ interactions",
                    "basis": "Basic",
                    "displacement": "Continuous variables",
                    "squeezing": "Continuous variables",
                    "data_reuploading": "Multiple layers",
                    "feature_map": "Pauli operators"
                }[encoding]
            }

        except Exception as e:
            analysis_results[encoding] = {"error": str(e)}

    return analysis_results


def optimize_circuit_configuration(target_params: int, target_depth: int,
                                 expressivity_priority: float = 0.5,
                                 hardware_priority: float = 0.3,
                                 pkpd_priority: float = 0.2) -> Dict:
    """
    Optimize circuit configuration based on priorities

    Args:
        target_params: Target number of parameters
        target_depth: Target circuit depth
        expressivity_priority: Weight for expressivity (0-1)
        hardware_priority: Weight for hardware friendliness (0-1)
        pkpd_priority: Weight for PK/PD suitability (0-1)

    Returns:
        Optimized configuration recommendations
    """
    # Normalize priorities
    total_priority = expressivity_priority + hardware_priority + pkpd_priority
    if total_priority > 0:
        expressivity_priority /= total_priority
        hardware_priority /= total_priority
        pkpd_priority /= total_priority

    # Test different configurations
    configurations = []

    for n_qubits in range(2, 8):  # Test 2-7 qubits
        for n_layers in range(1, min(10, target_depth + 3)):  # Test reasonable layer ranges
            comparison = compare_ansatz_complexity(n_qubits, n_layers)

            for ansatz_type, metrics in comparison.items():
                if 'error' in metrics:
                    continue

                # Calculate weighted score
                score = 0.0

                if 'expressivity_score' in metrics:
                    score += expressivity_priority * metrics['expressivity_score']

                # Hardware friendliness score (convert to numeric)
                hw_scores = {"Very High": 1.0, "High": 0.8, "Medium-High": 0.6, "Medium": 0.4, "Low": 0.2}
                hw_score = hw_scores.get(metrics.get('hardware_friendliness', 'Medium'), 0.4)
                score += hardware_priority * hw_score

                # PK/PD suitability score
                pkpd_scores = {"Very High": 1.0, "High": 0.8, "Medium-High": 0.6, "Medium": 0.4, "Low": 0.2}
                pkpd_score = pkpd_scores.get(metrics.get('pkpd_suitability', 'Medium'), 0.4)
                score += pkpd_priority * pkpd_score

                # Penalty for being far from target parameters/depth
                param_penalty = abs(metrics['total_parameters'] - target_params) / max(target_params, 1)
                depth_penalty = abs(metrics['estimated_depth'] - target_depth) / max(target_depth, 1)
                score -= 0.1 * (param_penalty + depth_penalty)

                configurations.append({
                    'ansatz': ansatz_type,
                    'n_qubits': n_qubits,
                    'n_layers': n_layers,
                    'score': score,
                    'metrics': metrics
                })

    # Sort by score and return top configurations
    configurations.sort(key=lambda x: x['score'], reverse=True)

    return {
        'top_configurations': configurations[:5],
        'optimization_criteria': {
            'expressivity_weight': expressivity_priority,
            'hardware_weight': hardware_priority,
            'pkpd_weight': pkpd_priority,
            'target_params': target_params,
            'target_depth': target_depth
        }
    }


def generate_benchmarking_report(circuit: 'VQCircuit', features_batch: np.ndarray,
                               target_outputs: Optional[np.ndarray] = None) -> str:
    """
    Generate a comprehensive benchmarking report for the VQ circuit

    Args:
        circuit: VQCircuit instance
        features_batch: Batch of features for testing
        target_outputs: Optional target outputs for accuracy assessment

    Returns:
        Formatted benchmarking report
    """
    report = []
    report.append("=" * 60)
    report.append("VQCdd Phase 2B - Quantum Circuit Benchmarking Report")
    report.append("=" * 60)

    # Circuit Configuration
    report.append("\n1. CIRCUIT CONFIGURATION")
    report.append("-" * 30)
    circuit_info = circuit.get_circuit_info()
    for key, value in circuit_info.items():
        if isinstance(value, (list, tuple)):
            value = str(value)
        report.append(f"{key}: {value}")

    # Resource Estimation
    report.append("\n2. RESOURCE ESTIMATION")
    report.append("-" * 30)
    resources = estimate_circuit_resources(circuit.config)
    for key, value in resources.items():
        report.append(f"{key}: {value}")

    # Performance Analysis
    if circuit.config.performance_analysis:
        report.append("\n3. PERFORMANCE ANALYSIS")
        report.append("-" * 30)
        perf_results = circuit.performance_analysis(features_batch, target_outputs)

        if 'execution_time' in perf_results:
            report.append(f"Average execution time: {perf_results['execution_time']['avg_time_per_sample']:.6f} seconds")
            report.append(f"Throughput: {perf_results['execution_time']['samples_per_second']:.2f} samples/second")

        if 'accuracy_metrics' in perf_results:
            report.append(f"MSE: {perf_results['accuracy_metrics']['mse']:.6f}")
            report.append(f"MAE: {perf_results['accuracy_metrics']['mae']:.6f}")
            report.append(f"RÂ² Score: {perf_results['accuracy_metrics']['r2_score']:.4f}")

    # Encoding Comparison
    if circuit.config.encoding_comparison:
        report.append("\n4. ENCODING STRATEGY COMPARISON")
        report.append("-" * 30)
        encoding_results = circuit.compare_encoding_strategies(features_batch[:10], target_outputs[:10] if target_outputs is not None else None)

        for encoding, metrics in encoding_results.items():
            if 'error' not in metrics:
                report.append(f"{encoding}:")
                report.append(f"  Expressivity: {metrics.get('expressivity', 'N/A'):.4f}")
                report.append(f"  Output variance: {metrics.get('output_variance', 'N/A'):.4f}")
                if 'mse' in metrics:
                    report.append(f"  MSE: {metrics['mse']:.6f}")

    # Expressivity Benchmarking
    if circuit.config.expressivity_benchmarking:
        report.append("\n5. ANSATZ EXPRESSIVITY BENCHMARKING")
        report.append("-" * 30)
        expr_results = circuit.benchmark_ansatz_expressivity()

        for ansatz, metrics in expr_results.items():
            if 'error' not in metrics:
                report.append(f"{ansatz}:")
                report.append(f"  Expressivity score: {metrics.get('expressivity_score', 'N/A'):.4f}")
                report.append(f"  Parameters: {metrics.get('parameter_count', 'N/A')}")
                report.append(f"  Efficiency: {metrics.get('expressivity_per_param', 'N/A'):.6f}")

    # Adaptive Depth Analysis
    if circuit.adaptive_depth_manager:
        report.append("\n6. ADAPTIVE DEPTH ANALYSIS")
        report.append("-" * 30)
        summary = circuit.adaptive_depth_manager.get_training_summary()
        report.append(f"Current depth: {summary['current_depth']}")
        report.append(f"Depth range: {summary['depth_range']}")
        report.append(f"Total adjustments: {summary['total_adjustments']}")
        report.append(f"Training iterations: {summary['training_iterations']}")

    report.append("\n" + "=" * 60)
    report.append("End of Benchmarking Report")
    report.append("=" * 60)

    return "\n".join(report)


if __name__ == "__main__":
    # Phase 2B Enhanced Example Usage and Testing
    print("VQCdd Phase 2B - Enhanced Quantum Circuit Module")
    print("=" * 50)

    # Test basic functionality with backward compatibility
    print("1. BASIC FUNCTIONALITY TEST (Backward Compatibility)")
    print("-" * 30)
    config = CircuitConfig(n_qubits=4, n_layers=2, ansatz="ry_cnot")
    circuit = VQCircuit(config)

    params = circuit.initialize_parameters(seed=42)
    features = np.array([24.0, 5.0, 70.0, 0.0])  # 24h, 5mg, 70kg, no comed
    outputs = circuit.forward(params, features)

    print(f"Circuit info: {circuit.get_circuit_info()}")
    print(f"Example outputs: {outputs}")
    print(f"Output range: [{outputs.min():.3f}, {outputs.max():.3f}]")

    # Test Phase 2B new ansatz types
    print("\n2. PHASE 2B NEW ANSATZ TYPES")
    print("-" * 30)
    new_ansatze = ["optimized_hardware_efficient", "qaoa_inspired", "pkpd_specific"]

    for ansatz_type in new_ansatze:
        try:
            config_new = CircuitConfig(n_qubits=4, n_layers=2, ansatz=ansatz_type)
            circuit_new = VQCircuit(config_new)
            params_new = circuit_new.initialize_parameters(seed=42)
            outputs_new = circuit_new.forward(params_new, features)
            print(f"{ansatz_type}: {len(outputs_new)} outputs, params shape: {params_new.shape}")
        except Exception as e:
            print(f"{ansatz_type}: Error - {e}")

    # Test enhanced encoding strategies
    print("\n3. ENHANCED ENCODING STRATEGIES")
    print("-" * 30)
    encoding_strategies = ["data_reuploading", "feature_map", "displacement", "squeezing"]

    for encoding in encoding_strategies:
        try:
            config_enc = CircuitConfig(n_qubits=4, n_layers=2, encoding=encoding)
            circuit_enc = VQCircuit(config_enc)
            params_enc = circuit_enc.initialize_parameters(seed=42)
            outputs_enc = circuit_enc.forward(params_enc, features)
            print(f"{encoding}: {len(outputs_enc)} outputs, range: [{outputs_enc.min():.3f}, {outputs_enc.max():.3f}]")
        except Exception as e:
            print(f"{encoding}: Error - {e}")

    # Test adaptive depth functionality
    print("\n4. ADAPTIVE DEPTH FUNCTIONALITY")
    print("-" * 30)
    config_adaptive = CircuitConfig(
        n_qubits=4,
        n_layers=3,
        ansatz="pkpd_specific",
        adaptive_depth=True,
        min_layers=1,
        max_layers=5,
        layer_wise_training=True
    )

    circuit_adaptive = VQCircuit(config_adaptive)
    if circuit_adaptive.adaptive_depth_manager:
        print(f"Initial depth: {circuit_adaptive.adaptive_depth_manager.current_depth}")
        print(f"Depth range: {config_adaptive.min_layers}-{config_adaptive.max_layers}")

        # Simulate training step
        params_adapt = circuit_adaptive.initialize_parameters()
        outputs_adapt = circuit_adaptive.forward(params_adapt, features)
        gradients = np.random.normal(0, 0.001, params_adapt.shape)  # Mock gradients
        loss = 0.1

        new_params, depth_changed = circuit_adaptive.adaptive_training_step(
            params_adapt, features, loss, gradients
        )
        print(f"Depth changed: {depth_changed}")
        print(f"New param shape: {new_params.shape}")

    # Enhanced comparison with Phase 2B features
    print("\n5. ENHANCED ANSATZ COMPARISON")
    print("-" * 30)
    comparison = compare_ansatz_complexity(4, 2)
    for ansatz, info in comparison.items():
        if 'error' not in info:
            print(f"{ansatz}:")
            print(f"  Parameters: {info['total_parameters']}")
            print(f"  Expressivity: {info.get('expressivity', 'N/A')} ({info.get('expressivity_score', 'N/A')})")
            print(f"  PK/PD Suitability: {info.get('pkpd_suitability', 'N/A')}")
            print(f"  Hardware Friendliness: {info.get('hardware_friendliness', 'N/A')}")

    # Test benchmarking capabilities
    print("\n6. BENCHMARKING AND ANALYSIS")
    print("-" * 30)
    config_bench = CircuitConfig(
        n_qubits=4,
        n_layers=2,
        ansatz="pkpd_specific",
        encoding="feature_map",
        encoding_comparison=True,
        expressivity_benchmarking=True,
        performance_analysis=True
    )

    circuit_bench = VQCircuit(config_bench)

    # Generate test data
    n_samples = 20
    features_batch = np.random.uniform(0, 1, (n_samples, 4))
    features_batch[:, 0] *= 48  # Time: 0-48 hours
    features_batch[:, 1] *= 10  # Dose: 0-10 mg
    features_batch[:, 2] = features_batch[:, 2] * 50 + 50  # Weight: 50-100 kg
    features_batch[:, 3] = np.round(features_batch[:, 3])  # Concomitant: 0 or 1

    try:
        # Generate comprehensive benchmarking report
        report = generate_benchmarking_report(circuit_bench, features_batch)
        print("Benchmarking report generated successfully!")
        print("First 500 characters of report:")
        print(report[:500] + "..." if len(report) > 500 else report)
    except Exception as e:
        print(f"Benchmarking error: {e}")

    # Test configuration optimization
    print("\n7. CONFIGURATION OPTIMIZATION")
    print("-" * 30)
    try:
        optimization_result = optimize_circuit_configuration(
            target_params=24,
            target_depth=6,
            expressivity_priority=0.4,
            hardware_priority=0.3,
            pkpd_priority=0.3
        )

        print("Top 3 recommended configurations:")
        for i, config in enumerate(optimization_result['top_configurations'][:3]):
            print(f"{i+1}. {config['ansatz']} ({config['n_qubits']}q, {config['n_layers']}l)")
            print(f"   Score: {config['score']:.4f}")
            print(f"   Parameters: {config['metrics']['total_parameters']}")

    except Exception as e:
        print(f"Configuration optimization error: {e}")

    print(f"\n{'-' * 50}")
    print("Phase 2B Testing Complete!")
    print("âœ“ Alternative ansatz architectures implemented")
    print("âœ“ Adaptive circuit depth functionality added")
    print("âœ“ Enhanced encoding strategies available")
    print("âœ“ Comprehensive benchmarking tools integrated")
    print("âœ“ Backward compatibility maintained")
    print(f"{'-' * 50}")