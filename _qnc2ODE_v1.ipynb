{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf43a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete Dosing Mechanistic PK-PD Model with Neural Parameter Estimation\n",
    "# Complete rewrite with proper discrete dose handling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdiffeq import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6aa346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set device and random seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b3595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def torch_interp(x, xp, fp):\n",
    "    \"\"\"Simple linear interpolation for PyTorch tensors\"\"\"\n",
    "    if len(xp) == 0:\n",
    "        return torch.zeros_like(x)\n",
    "    if len(xp) == 1:\n",
    "        return fp[0].expand_as(x)\n",
    "    \n",
    "    indices = torch.searchsorted(xp, x, right=False)\n",
    "    indices = torch.clamp(indices, 0, len(xp) - 1)\n",
    "    indices_left = torch.clamp(indices - 1, 0, len(xp) - 1)\n",
    "    indices_right = torch.clamp(indices, 0, len(xp) - 1)\n",
    "    \n",
    "    x_left = xp[indices_left]\n",
    "    x_right = xp[indices_right] \n",
    "    y_left = fp[indices_left]\n",
    "    y_right = fp[indices_right]\n",
    "    \n",
    "    dx = x_right - x_left\n",
    "    dx = torch.where(dx == 0, torch.ones_like(dx), dx)\n",
    "    \n",
    "    weights = (x - x_left) / dx\n",
    "    result = y_left + weights * (y_right - y_left)\n",
    "    \n",
    "    result = torch.where(x < xp[0], fp[0], result)\n",
    "    result = torch.where(x > xp[-1], fp[-1], result)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750c88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: DATA PREPARATION (unchanged from original)\n",
    "# ============================================================================\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Handle all data loading, cleaning, and preparation tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path='data/EstData.csv', device=device):\n",
    "        self.data_path = data_path\n",
    "        self.device = device\n",
    "        self.df = None\n",
    "        self.pk_data = None\n",
    "        self.pd_data = None\n",
    "        self.dosing_data = None\n",
    "        self.subject_info = None\n",
    "        \n",
    "    def load_and_examine_data(self):\n",
    "        \"\"\"Load data and perform initial examination\"\"\"\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"SECTION 1: DATA PREPARATION - DISCRETE DOSING VERSION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"Loading data from {self.data_path}...\")\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        \n",
    "        # Basic data info\n",
    "        print(f\"\\nData Overview:\")\n",
    "        print(f\"  Total records: {len(self.df)}\")\n",
    "        print(f\"  Subjects: {self.df['ID'].nunique()}\")\n",
    "        print(f\"  Columns: {list(self.df.columns)}\")\n",
    "        \n",
    "        # Separate data types\n",
    "        self.pk_data = self.df[(self.df['DVID'] == 1) & (self.df['EVID'] == 0)].copy()\n",
    "        self.pd_data = self.df[(self.df['DVID'] == 2) & (self.df['EVID'] == 0)].copy()\n",
    "        self.dosing_data = self.df[self.df['EVID'] == 1].copy()\n",
    "        \n",
    "        print(f\"\\nData Breakdown:\")\n",
    "        print(f\"  PK observations (DVID=1): {len(self.pk_data)}\")\n",
    "        print(f\"  PD observations (DVID=2): {len(self.pd_data)}\")\n",
    "        print(f\"  Dosing events (EVID=1): {len(self.dosing_data)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def quality_checks(self):\n",
    "        \"\"\"Perform data quality assessment\"\"\"\n",
    "        \n",
    "        print(\"\\nData Quality Checks:\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_counts = self.df.isnull().sum()\n",
    "        if missing_counts.any():\n",
    "            print(\"  Missing values found:\")\n",
    "            print(missing_counts[missing_counts > 0])\n",
    "        else:\n",
    "            print(\"  ✓ No missing values\")\n",
    "        \n",
    "        # Check dose distribution\n",
    "        dose_dist = self.df['DOSE'].value_counts().sort_index()\n",
    "        print(f\"  Dose distribution: {dose_dist.to_dict()}\")\n",
    "        \n",
    "        # Check covariate ranges\n",
    "        print(f\"  Body weight range: {self.df['BW'].min():.1f} - {self.df['BW'].max():.1f} kg\")\n",
    "        print(f\"  COMED distribution: {self.df['COMED'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Check time ranges\n",
    "        print(f\"  Time range: {self.df['TIME'].min():.0f} - {self.df['TIME'].max():.0f} hours\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_subject_summaries(self):\n",
    "        \"\"\"Create subject-level information\"\"\"\n",
    "        \n",
    "        print(\"\\nCreating subject summaries...\")\n",
    "        \n",
    "        subjects = []\n",
    "        for subject_id in self.df['ID'].unique():\n",
    "            subj_data = self.df[self.df['ID'] == subject_id]\n",
    "            \n",
    "            # Basic info\n",
    "            bw = subj_data['BW'].iloc[0]\n",
    "            comed = subj_data['COMED'].iloc[0]\n",
    "            dose = subj_data['DOSE'].iloc[0]\n",
    "            \n",
    "            # PK data\n",
    "            subj_pk = self.pk_data[self.pk_data['ID'] == subject_id]\n",
    "            pk_observations = len(subj_pk)\n",
    "            \n",
    "            # PD data\n",
    "            subj_pd = self.pd_data[self.pd_data['ID'] == subject_id]\n",
    "            pd_observations = len(subj_pd)\n",
    "            \n",
    "            # Dosing data\n",
    "            subj_dosing = self.dosing_data[self.dosing_data['ID'] == subject_id]\n",
    "            n_doses = len(subj_dosing)\n",
    "            \n",
    "            subjects.append({\n",
    "                'ID': subject_id,\n",
    "                'BW': bw,\n",
    "                'COMED': comed,\n",
    "                'DOSE': dose,\n",
    "                'PK_obs': pk_observations,\n",
    "                'PD_obs': pd_observations,\n",
    "                'N_doses': n_doses\n",
    "            })\n",
    "        \n",
    "        self.subject_info = pd.DataFrame(subjects)\n",
    "        \n",
    "        print(f\"  Subject summary created for {len(subjects)} subjects\")\n",
    "        print(f\"  Dose group sizes: {self.subject_info.groupby('DOSE').size().to_dict()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def train_validation_split(self, val_fraction=0.2):\n",
    "        \"\"\"Split subjects into training and validation sets\"\"\"\n",
    "        \n",
    "        print(f\"\\nSplitting data (validation fraction: {val_fraction})...\")\n",
    "        \n",
    "        # Stratified split by dose group to ensure representation\n",
    "        train_subjects, val_subjects = [], []\n",
    "        \n",
    "        for dose in self.subject_info['DOSE'].unique():\n",
    "            dose_subjects = self.subject_info[self.subject_info['DOSE'] == dose]['ID'].values\n",
    "            \n",
    "            n_val = max(1, int(len(dose_subjects) * val_fraction))\n",
    "            val_dose_subjects = np.random.choice(dose_subjects, n_val, replace=False)\n",
    "            train_dose_subjects = [s for s in dose_subjects if s not in val_dose_subjects]\n",
    "            \n",
    "            train_subjects.extend(train_dose_subjects)\n",
    "            val_subjects.extend(val_dose_subjects)\n",
    "        \n",
    "        self.train_subjects = train_subjects\n",
    "        self.val_subjects = val_subjects\n",
    "        \n",
    "        print(f\"  Training subjects: {len(train_subjects)}\")\n",
    "        print(f\"  Validation subjects: {len(val_subjects)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_subject_data(self, subject_id):\n",
    "        \"\"\"Get all data for a specific subject\"\"\"\n",
    "        \n",
    "        subj_info = self.subject_info[self.subject_info['ID'] == subject_id].iloc[0]\n",
    "        \n",
    "        return {\n",
    "            'id': subject_id,\n",
    "            'covariates': torch.tensor([subj_info['BW'], subj_info['COMED']], dtype=torch.float32, device=self.device),\n",
    "            'dose': subj_info['DOSE'],\n",
    "            'pk_data': self.pk_data[self.pk_data['ID'] == subject_id].copy(),\n",
    "            'pd_data': self.pd_data[self.pd_data['ID'] == subject_id].copy(),\n",
    "            'dosing_data': self.dosing_data[self.dosing_data['ID'] == subject_id].copy()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ba7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: NEURAL NETWORK ARCHITECTURE (simplified for stability)\n",
    "# ============================================================================\n",
    "\n",
    "class DiscreteNeuralParameterEstimator(nn.Module):\n",
    "    def __init__(self, n_subjects, embedding_dim=16, device='cpu'):  # Keep signature for compatibility\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Remove this line:\n",
    "        # self.subject_embeddings = nn.Embedding(n_subjects, embedding_dim)\n",
    "        \n",
    "        # Keep population parameters unchanged\n",
    "        self.pop_pk_params = nn.Parameter(torch.tensor([2.0, 6.0, 45.0, 8.0, 90.0]))\n",
    "        self.pop_pd_params = nn.Parameter(torch.tensor([15.0, 1.5, 0.75, 5.0]))\n",
    "        \n",
    "        # Single layer MLP with reduced input\n",
    "        input_dim = 3  # Only BW + COMED + dose_intensity\n",
    "        # self.param_net = nn.Sequential(\n",
    "        #     nn.Linear(input_dim, 9),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "        self.param_net = nn.Sequential(\n",
    "        nn.Linear(input_dim, 9),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(9, 9),\n",
    "        nn.Sigmoid()  # Final activation\n",
    "        )\n",
    "        # Define physiologically reasonable bounds\n",
    "        self.pk_bounds = torch.tensor([\n",
    "            [0.5, 5.0],    # Ka: 0.5-5.0 /h\n",
    "            [2.0, 20.0],   # CL: 2-20 L/h\n",
    "            [20.0, 100.0], # Vc: 20-100 L\n",
    "            [2.0, 30.0],   # Q: 2-30 L/h\n",
    "            [40.0, 200.0]  # Vp: 40-200 L\n",
    "        ], device=self.device)\n",
    "        \n",
    "        self.pd_bounds = torch.tensor([\n",
    "            [5.0, 25.0],   # Kin: 5-25 ng/mL/h\n",
    "            [0.3, 3.0],    # Kout: 0.3-3.0 /h\n",
    "            [0.3, 0.95],   # Imax: 30-95% max inhibition\n",
    "            [1.0, 15.0]    # IC50: 1-15 ng/mL\n",
    "        ], device=self.device)\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        print(f\"Neural Network Architecture:\")\n",
    "        print(f\"  Subjects: {n_subjects}\")\n",
    "        print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "        print(f\"  Network: {input_dim}  → 9\")\n",
    "        print(f\"  Total parameters: {sum(p.numel() for p in self.parameters())}\")\n",
    "        self.to(self.device)\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize single layer to output factors ≈ 1.0\"\"\"\n",
    "        \n",
    "        # Remove subject embedding initialization\n",
    "        \n",
    "        # Initialize single layer\n",
    "        # For sigmoid outputs, initialize to produce mid-range parameters\n",
    "        for m in self.param_net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight, gain=0.1)\n",
    "                nn.init.constant_(m.bias, 0.0)  # Sigmoid(0) = 0.5 → mid-range parameters\n",
    "        \n",
    "    def forward(self, subject_ids, covariates, dose_intensities):\n",
    "        \"\"\"Forward pass with only covariates - no subject effects\"\"\"\n",
    "        \n",
    "        inputs = torch.cat([covariates, dose_intensities.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        # Predict raw parameters\n",
    "        raw_params = self.param_net(inputs)\n",
    "        \n",
    "        # Apply sigmoid to map to [0,1], then scale to bounds\n",
    "        pk_raw = raw_params[:, :5]\n",
    "        pd_raw = raw_params[:, 5:]\n",
    "        \n",
    "        # Scale to physiological ranges\n",
    "        pk_params = self.pk_bounds[:, 0] + pk_raw * (self.pk_bounds[:, 1] - self.pk_bounds[:, 0])\n",
    "        pd_params = self.pd_bounds[:, 0] + pd_raw * (self.pd_bounds[:, 1] - self.pd_bounds[:, 0])\n",
    "        # Apply multiplicative scaling to population parameters\n",
    "        # pk_params = self.pop_pk_params.unsqueeze(0) * pk_factors\n",
    "        # pd_params = self.pop_pd_params.unsqueeze(0) * pd_factors\n",
    "    \n",
    "        # Constrain Imax and ensure reasonable Kout\n",
    "        # pd_params = torch.cat([\n",
    "        #     pd_params[:, :1],  # Kin\n",
    "        #     torch.clamp(pd_params[:, 1:2], min=0.5, max=3.0),  # Kout: reasonable range\n",
    "        #     torch.clamp(pd_params[:, 2:3], max=0.9),  # Imax < 1\n",
    "        #     pd_params[:, 3:]   # IC50\n",
    "        # ], dim=1)\n",
    "\n",
    "        return pk_params, pd_params\n",
    "    \n",
    "    def predict_single_subject(self, subject_id, bw, comed, dose_intensity):\n",
    "        \"\"\"Single subject prediction - subject_id no longer used\"\"\"\n",
    "        \n",
    "        # subject_id is ignored now\n",
    "        covariates = torch.tensor([[bw, comed]], dtype=torch.float32, device=self.device)\n",
    "        dose_intensities = torch.tensor([dose_intensity], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        was_training = self.training\n",
    "        self.eval()\n",
    "        \n",
    "        # Pass dummy subject_ids for compatibility\n",
    "        dummy_subject_ids = torch.tensor([0], device=self.device)\n",
    "        pk_params, pd_params = self.forward(dummy_subject_ids, covariates, dose_intensities)\n",
    "        \n",
    "        self.train(was_training)\n",
    "        return pk_params[0], pd_params[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2fc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: DISCRETE DOSING MECHANISTIC MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class DiscreteDosePKPDModel(nn.Module):\n",
    "    \"\"\"Mechanistic PK-PD model with discrete dosing - no rate spikes\"\"\"\n",
    "    \n",
    "    def __init__(self, param_estimator, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.param_estimator = param_estimator\n",
    "        self.device = device\n",
    "\n",
    "        print(\"=\"*80)\n",
    "        print(\"SECTION 3: DISCRETE DOSING MECHANISTIC MODEL\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"Mechanistic Model Components:\")\n",
    "        print(\"  PK Model: Two-compartment with first-order absorption\")\n",
    "        print(\"  PD Model: Indirect response (inhibition of production)\")\n",
    "        print(\"  Dosing: Discrete doses added directly to depot compartment\")\n",
    "        print(\"  NO continuous dose rates or numerical spikes\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def ode_system_no_dosing(self, t, y, params):\n",
    "        \"\"\"ODE system without dosing - eliminates rate spikes completely\"\"\"\n",
    "        \n",
    "        A_depot, A_central, A_periph, R = y\n",
    "        Ka, CL, Vc, Q, Vp, Kin, Kout, Imax, IC50 = params\n",
    "        \n",
    "        # PK equations (no dose input)\n",
    "        dA_depot = -Ka * A_depot\n",
    "        dA_central = Ka * A_depot - (CL/Vc) * A_central - (Q/Vc) * A_central + (Q/Vp) * A_periph\n",
    "        dA_periph = (Q/Vc) * A_central - (Q/Vp) * A_periph\n",
    "        \n",
    "        # PD equation\n",
    "        concentration = A_central / Vc\n",
    "        inhibition = Imax * concentration / (IC50 + concentration + 1e-6)\n",
    "        dR = Kin * (1 - inhibition) - Kout * R\n",
    "        \n",
    "        return torch.stack([dA_depot, dA_central, dA_periph, dR])\n",
    "    \n",
    "    # def simulate_subject_discrete(self, subject_id, covariates, dose_schedule, simulation_times):\n",
    "    #     \"\"\"Simulate with discrete dosing - stable and reliable\"\"\"\n",
    "        \n",
    "    #     # Get individual parameters\n",
    "    #     total_dose = sum(dose_amt for _, dose_amt in dose_schedule)\n",
    "    #     dose_intensity = total_dose / covariates[0]\n",
    "        \n",
    "    #     pk_params, pd_params = self.param_estimator.predict_single_subject(\n",
    "    #         subject_id, covariates[0], covariates[1], dose_intensity)\n",
    "        \n",
    "    #     all_params = torch.cat([pk_params, pd_params])\n",
    "        \n",
    "    #     # Initial conditions\n",
    "    #     baseline_biomarker = 16.0 * (1 + 0.1 * covariates[1])  # COMED effect\n",
    "    #     y = torch.tensor([0.0, 0.0, 0.0, baseline_biomarker])\n",
    "        \n",
    "    #     # Create timeline with all important time points\n",
    "    #     dose_times = [float(t) for t, _ in dose_schedule]\n",
    "    #     sim_times_list = simulation_times.tolist()\n",
    "    #     all_important_times = sorted(set([0.0] + dose_times + sim_times_list))\n",
    "        \n",
    "    #     # Solve in segments between dose times\n",
    "    #     solutions = {}\n",
    "    #     solutions[0.0] = y\n",
    "        \n",
    "    #     for i in range(len(all_important_times) - 1):\n",
    "    #         t_start = all_important_times[i]\n",
    "    #         t_end = all_important_times[i + 1]\n",
    "            \n",
    "    #         # Create time points for this segment\n",
    "    #         segment_times = torch.tensor([t_start, t_end], dtype=torch.float32)\n",
    "    #         y_start = solutions[t_start]\n",
    "            \n",
    "    #         # Solve ODE segment with no dosing\n",
    "    #         try:\n",
    "    #             segment_solution = odeint(\n",
    "    #                 lambda t, state: self.ode_system_no_dosing(t, state, all_params),\n",
    "    #                 y_start,\n",
    "    #                 segment_times,\n",
    "    #                 method='euler',\n",
    "    #                 options={'step_size': 1.0},\n",
    "    #                 rtol=1e-5,\n",
    "    #                 atol=1e-6\n",
    "    #             )\n",
    "                \n",
    "    #             y_end = segment_solution[-1]\n",
    "                \n",
    "    #             # Check for numerical issues\n",
    "    #             if torch.isnan(y_end).any() or torch.isinf(y_end).any():\n",
    "    #                 print(f\"Warning: Numerical issue at t={t_end}\")\n",
    "    #                 # Use simple forward Euler as fallback\n",
    "    #                 dt = t_end - t_start\n",
    "    #                 dydt = self.ode_system_no_dosing(t_start, y_start, all_params)\n",
    "    #                 y_end = y_start + dt * dydt\n",
    "                \n",
    "    #         except Exception as e:\n",
    "    #             print(f\"ODE solver failed, using Euler step: {e}\")\n",
    "    #             dt = t_end - t_start\n",
    "    #             dydt = self.ode_system_no_dosing(t_start, y_start, all_params)\n",
    "    #             y_end = y_start + dt * dydt\n",
    "            \n",
    "    #         solutions[t_end] = y_end\n",
    "            \n",
    "    #         # Add discrete dose if this is a dose time\n",
    "    #         dose_amount = 0.0\n",
    "    #         for dose_time, dose_amt in dose_schedule:\n",
    "    #             if abs(t_end - dose_time) < 0.001:  # Floating point comparison\n",
    "    #                 dose_amount += dose_amt\n",
    "            \n",
    "    #         if dose_amount > 0:\n",
    "    #             solutions[t_end] = solutions[t_end].clone()  # Make it writable\n",
    "    #             solutions[t_end][0] += dose_amount  # Add to depot compartment\n",
    "        \n",
    "    #     # Extract solutions at requested simulation times\n",
    "    #     final_solution = torch.zeros(len(simulation_times), 4)\n",
    "        \n",
    "    #     for i, sim_time in enumerate(simulation_times):\n",
    "    #         t = float(sim_time)\n",
    "    #         if t in solutions:\n",
    "    #             final_solution[i] = solutions[t]\n",
    "    #         else:\n",
    "    #             # Linear interpolation between nearest points\n",
    "    #             t_before = max([time for time in solutions.keys() if time <= t])\n",
    "    #             t_after = min([time for time in solutions.keys() if time >= t])\n",
    "                \n",
    "    #             if t_before == t_after:\n",
    "    #                 final_solution[i] = solutions[t_before]\n",
    "    #             else:\n",
    "    #                 # Linear interpolation\n",
    "    #                 alpha = (t - t_before) / (t_after - t_before)\n",
    "    #                 final_solution[i] = (1 - alpha) * solutions[t_before] + alpha * solutions[t_after]\n",
    "        \n",
    "    #     return final_solution\n",
    "    def simulate_subject_discrete(self, subject_id, covariates, dose_schedule, simulation_times, baseline_biomarker=16.0):\n",
    "        \"\"\"\n",
    "        Simulate PK-PD response with discrete dosing using chronological integration\n",
    "        \n",
    "        Args:\n",
    "            subject_id: Subject identifier for parameter prediction\n",
    "            covariates: [body_weight, comed_status]\n",
    "            dose_schedule: List of (time, dose_amount) tuples\n",
    "            simulation_times: Times where output is needed\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Get individual PK-PD parameters\n",
    "        total_dose = sum(dose_amt for _, dose_amt in dose_schedule)\n",
    "        dose_intensity = total_dose / covariates[0]  # We can discuss removing this later\n",
    "        \n",
    "        pk_params, pd_params = self.param_estimator.predict_single_subject(\n",
    "            subject_id, covariates[0], covariates[1], dose_intensity)\n",
    "        all_params = torch.cat([pk_params, pd_params])\n",
    "        \n",
    "        \n",
    "        # Step 3: Initialize system state [depot, central, peripheral, biomarker]\n",
    "        current_state = torch.tensor([0.0, 0.0, 0.0, baseline_biomarker], device=self.device)\n",
    "        #                [depot_amount, central_amount, peripheral_amount, biomarker_concentration]\n",
    "        #                [mg,           mg,             mg,                ng/mL]\n",
    "        current_time = 0.0\n",
    "        \n",
    "        # Step 4: Create sorted event timeline\n",
    "        dose_events = sorted(dose_schedule, key=lambda x: x[0])  # Sort by time\n",
    "        max_time = max(simulation_times.max().item(), \n",
    "                    dose_events[-1][0] if dose_events else 0.0)\n",
    "        \n",
    "        # Step 5: Storage for solutions at all time points\n",
    "        solution_dict = {0.0: current_state.clone()}\n",
    "        \n",
    "        # Step 6: Process each dosing interval chronologically\n",
    "        for i, (dose_time, dose_amount) in enumerate(dose_events):\n",
    "            \n",
    "            # 6a: Integrate from current_time to dose_time (no dosing during integration)\n",
    "            if dose_time > current_time:\n",
    "                integration_times = torch.tensor([current_time, dose_time], dtype=torch.float32, device=self.device)\n",
    "                \n",
    "                try:\n",
    "                    ode_solution = odeint(\n",
    "                        lambda t, y: self.ode_system_no_dosing(t, y, all_params),\n",
    "                        current_state,\n",
    "                        integration_times,\n",
    "                        # method='rk4',\n",
    "                        # options={'step_size': 1.0},\n",
    "                        method='dopri5',  # Changed to euler for stability\n",
    "                        rtol=1e-2,\n",
    "                        atol=1e-3\n",
    "                    )\n",
    "                    current_state = ode_solution[-1]  # State just before dose\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"ODE integration failed at t={dose_time}: {e}\")\n",
    "                    # Simple Euler fallback\n",
    "                    dt = dose_time - current_time\n",
    "                    dydt = self.ode_system_no_dosing(current_time, current_state, all_params)\n",
    "                    current_state = current_state + dt * dydt\n",
    "            \n",
    "            # 6b: Store pre-dose state\n",
    "            solution_dict[float(dose_time)] = current_state.clone()\n",
    "            \n",
    "            # 6c: Add discrete dose to depot compartment\n",
    "            current_state[0] += dose_amount  # Instantaneous dose addition\n",
    "            current_time = dose_time\n",
    "            \n",
    "            # 6d: Store post-dose state (same time, different state)\n",
    "            solution_dict[float(dose_time) + 1e-6] = current_state.clone()\n",
    "        \n",
    "        # Step 7: Integrate from last dose to simulation end if needed\n",
    "        if current_time < max_time:\n",
    "            final_times = torch.tensor([current_time, max_time], dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            try:\n",
    "                final_solution = odeint(\n",
    "                    lambda t, y: self.ode_system_no_dosing(t, y, all_params),\n",
    "                    current_state,\n",
    "                    final_times,\n",
    "                    method='dopri5',\n",
    "                    rtol=1e-3,\n",
    "                    atol=1e-4\n",
    "                )\n",
    "                solution_dict[float(max_time)] = final_solution[-1]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Final integration failed: {e}\")\n",
    "                solution_dict[float(max_time)] = current_state\n",
    "        \n",
    "        # Step 8: Extract solutions at requested simulation_times via interpolation\n",
    "        output_solution = torch.zeros(len(simulation_times), 4, device=self.device)\n",
    "        \n",
    "        # Get all available time points in order\n",
    "        available_times = sorted(solution_dict.keys())\n",
    "        \n",
    "        for i, req_time in enumerate(simulation_times):\n",
    "            t = float(req_time)\n",
    "            \n",
    "            if t in solution_dict:\n",
    "                # Exact time point available\n",
    "                output_solution[i] = solution_dict[t]\n",
    "            else:\n",
    "                # Linear interpolation between nearest points\n",
    "                t_before = max([time for time in available_times if time <= t], default=available_times[0])\n",
    "                t_after = min([time for time in available_times if time >= t], default=available_times[-1])\n",
    "                \n",
    "                if t_before == t_after:\n",
    "                    output_solution[i] = solution_dict[t_before]\n",
    "                else:\n",
    "                    # Linear interpolation\n",
    "                    alpha = (t - t_before) / (t_after - t_before)\n",
    "                    state_before = solution_dict[t_before]\n",
    "                    state_after = solution_dict[t_after]\n",
    "                    output_solution[i] = (1 - alpha) * state_before + alpha * state_after\n",
    "        \n",
    "        return output_solution, pk_params, pd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63de20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: TRAINING WITH DISCRETE DOSING\n",
    "# ============================================================================\n",
    "\n",
    "class DiscretePKPDTrainer:\n",
    "    \"\"\"Training framework optimized for discrete dosing model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, data_processor, device=device):\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"SECTION 4: DISCRETE DOSING NEURAL NETWORK TRAINING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.model = model.to(device)\n",
    "        self.data_processor = data_processor\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer with conservative settings\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.7)\n",
    "        \n",
    "        # Loss tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        # Pre-compute all baselines during initialization\n",
    "        self.subject_baselines = {}\n",
    "        for subject_id in data_processor.subject_info['ID']:\n",
    "            self.subject_baselines[subject_id] = self.get_baseline_biomarker(subject_id)\n",
    "\n",
    "        print(f\"Training Setup:\")\n",
    "        print(f\"  Device: {device}\")\n",
    "        print(f\"  Optimizer: Adam (lr=1e-3, weight_decay=1e-4)\")\n",
    "        print(f\"  Scheduler: StepLR (step_size=20, gamma=0.7)\")\n",
    "        print(f\"  Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    def get_baseline_biomarker(self, subject_id):\n",
    "        \"\"\"Get baseline from cache or compute if not cached\"\"\"\n",
    "        \n",
    "        # Use cache if available\n",
    "        if hasattr(self, 'subject_baselines') and subject_id in self.subject_baselines:\n",
    "            return self.subject_baselines[subject_id]\n",
    "        \n",
    "        # Compute baseline if not cached\n",
    "        try:\n",
    "            print('Computing baseline for subject', subject_id, ' (not cached)')\n",
    "            subject_pd = self.data_processor.df[\n",
    "                (self.data_processor.df['ID'] == subject_id) & \n",
    "                (self.data_processor.df['EVID'] == 0) & \n",
    "                (self.data_processor.df['DVID'] == 2)\n",
    "            ]\n",
    "            \n",
    "            if len(subject_pd) > 0:\n",
    "                earliest_idx = subject_pd['TIME'].idxmin()\n",
    "                return float(subject_pd.loc[earliest_idx, 'DV'])\n",
    "            else:\n",
    "                print(f\"Warning: No PD data for subject {subject_id} in the function get_baseline_biomarker, using default baseline\")\n",
    "                return 16.0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Baseline retrieval failed for subject {subject_id}: {e}\")\n",
    "            return 16.0\n",
    "        \n",
    "    def extract_baseline_distribution(self):\n",
    "        \"\"\"Extract baseline biomarker distribution from actual data\"\"\"\n",
    "        \n",
    "        baselines = []\n",
    "        subjects_with_baselines = []\n",
    "        \n",
    "        for subject_id in self.data_processor.subject_info['ID']:\n",
    "            try:\n",
    "                baseline = self.get_baseline_biomarker(subject_id)\n",
    "                baselines.append(baseline)\n",
    "                subjects_with_baselines.append(subject_id)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        baselines = np.array(baselines)\n",
    "        \n",
    "        print(f\"\\nBaseline Distribution Analysis:\")\n",
    "        print(f\"  Valid baselines: {len(baselines)} subjects\")\n",
    "        print(f\"  Range: {baselines.min():.2f} - {baselines.max():.2f} ng/mL\")\n",
    "        print(f\"  Mean: {baselines.mean():.2f} ng/mL\")\n",
    "        print(f\"  Median: {np.median(baselines):.2f} ng/mL\")\n",
    "        print(f\"  Std: {baselines.std():.2f} ng/mL\")\n",
    "        \n",
    "        # Store for use in dose optimization\n",
    "        self.baseline_distribution = baselines\n",
    "        return baselines\n",
    "    # def compute_loss_discrete(self, subject_data):\n",
    "    #     \"\"\"Compute loss with discrete dosing - much more stable\"\"\"\n",
    "        \n",
    "    #     subject_id = subject_data['id']\n",
    "    #     covariates = subject_data['covariates']\n",
    "    #     pk_obs = subject_data['pk_data']\n",
    "    #     pd_obs = subject_data['pd_data']\n",
    "    #     dosing = subject_data['dosing_data']\n",
    "        \n",
    "    #     if len(dosing) == 0:\n",
    "    #         return torch.tensor(0.0)\n",
    "        \n",
    "    #     # Create dose schedule\n",
    "    #     dose_schedule = [(row['TIME'], row['AMT']) for _, row in dosing.iterrows()]\n",
    "        \n",
    "    #     # Get observation times\n",
    "    #     obs_times = set()\n",
    "    #     if len(pk_obs) > 0:\n",
    "    #         obs_times.update(pk_obs['TIME'].values)\n",
    "    #     if len(pd_obs) > 0:\n",
    "    #         obs_times.update(pd_obs['TIME'].values)\n",
    "        \n",
    "    #     if not obs_times:\n",
    "    #         return torch.tensor(0.0)\n",
    "        \n",
    "    #     sim_times = torch.tensor(sorted(obs_times), dtype=torch.float32)\n",
    "        \n",
    "    #     # Simulate with discrete dosing\n",
    "    #     try:\n",
    "    #         solution = self.model.simulate_subject_discrete(\n",
    "    #             subject_id, covariates, dose_schedule, sim_times)\n",
    "            \n",
    "    #         # Check for numerical issues\n",
    "    #         if torch.isnan(solution).any() or torch.isinf(solution).any():\n",
    "    #             print(f\"Warning: Numerical instability in subject {subject_id}\")\n",
    "    #             return torch.tensor(100.0)\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Simulation failed for subject {subject_id}: {e}\")\n",
    "    #         return torch.tensor(100.0)\n",
    "        \n",
    "    #     total_loss = 0.0\n",
    "    #     n_terms = 0\n",
    "        \n",
    "    #     # PK loss (log scale) - if PK data available\n",
    "    #     if len(pk_obs) > 0:\n",
    "    #         total_dose = sum(amt for _, amt in dose_schedule)\n",
    "    #         dose_intensity = total_dose / covariates[0]\n",
    "    #         pk_params, _ = self.model.param_estimator.predict_single_subject(\n",
    "    #             subject_id, covariates[0], covariates[1], dose_intensity)\n",
    "    #         Vc = pk_params[2]\n",
    "            \n",
    "    #         pk_times_tensor = torch.tensor(pk_obs['TIME'].values, dtype=torch.float32)\n",
    "    #         pk_obs_values = torch.tensor(pk_obs['DV'].values, dtype=torch.float32)\n",
    "            \n",
    "    #         # Calculate concentrations\n",
    "    #         central_amounts = solution[:, 1]\n",
    "    #         concentrations = central_amounts / Vc\n",
    "            \n",
    "    #         pred_concentrations = torch_interp(pk_times_tensor, sim_times, concentrations)\n",
    "            \n",
    "    #         # Log-scale loss with numerical stability\n",
    "    #         pk_loss = nn.MSELoss()(\n",
    "    #             torch.log(pred_concentrations + 0.001),\n",
    "    #             torch.log(pk_obs_values + 0.001)\n",
    "    #         )\n",
    "            \n",
    "    #         total_loss += pk_loss\n",
    "    #         n_terms += 1\n",
    "    #     # PK loss (linear scale) - if PK data available\n",
    "    #     # if len(pk_obs) > 0:\n",
    "    #     #     total_dose = sum(amt for _, amt in dose_schedule)\n",
    "    #     #     dose_intensity = total_dose / covariates[0]\n",
    "    #     #     pk_params, _ = self.model.param_estimator.predict_single_subject(\n",
    "    #     #         subject_id, covariates[0], covariates[1], dose_intensity)\n",
    "    #     #     Vc = pk_params[2]\n",
    "            \n",
    "    #     #     pk_times_tensor = torch.tensor(pk_obs['TIME'].values, dtype=torch.float32)\n",
    "    #     #     pk_obs_values = torch.tensor(pk_obs['DV'].values, dtype=torch.float32)\n",
    "            \n",
    "    #     #     # Calculate concentrations\n",
    "    #     #     central_amounts = solution[:, 1]\n",
    "    #     #     concentrations = central_amounts / Vc\n",
    "            \n",
    "    #     #     pred_concentrations = torch_interp(pk_times_tensor, sim_times, concentrations)\n",
    "            \n",
    "    #     #     # Linear-scale loss (removed log transformation)\n",
    "    #     #     pk_loss = nn.MSELoss()(pred_concentrations, pk_obs_values)\n",
    "            \n",
    "    #     #     total_loss += pk_loss\n",
    "    #     #     n_terms += 1\n",
    "    #     # PD loss (linear scale)\n",
    "    #     if len(pd_obs) > 0:\n",
    "    #         pd_times_tensor = torch.tensor(pd_obs['TIME'].values, dtype=torch.float32)\n",
    "    #         pd_obs_values = torch.tensor(pd_obs['DV'].values, dtype=torch.float32)\n",
    "            \n",
    "    #         biomarker_levels = solution[:, 3]\n",
    "    #         pred_biomarkers = torch_interp(pd_times_tensor, sim_times, biomarker_levels)\n",
    "            \n",
    "    #         pd_loss = nn.MSELoss()(pred_biomarkers, pd_obs_values)\n",
    "    #         total_loss += pd_loss\n",
    "    #         n_terms += 1\n",
    "        \n",
    "    #     return total_loss / max(n_terms, 1)\n",
    "    def compute_loss_discrete(self, subject_data):\n",
    "        \"\"\"Compute loss with discrete dosing - eliminates redundant parameter prediction\"\"\"\n",
    "        \n",
    "        subject_id = subject_data['id']\n",
    "        covariates = subject_data['covariates']\n",
    "        pk_obs = subject_data['pk_data']\n",
    "        pd_obs = subject_data['pd_data']\n",
    "        dosing = subject_data['dosing_data']\n",
    "        \n",
    "        if len(dosing) == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Create dose schedule\n",
    "        dose_schedule = [(row['TIME'], row['AMT']) for _, row in dosing.iterrows()]\n",
    "        \n",
    "        # Get observation times\n",
    "        obs_times = set()\n",
    "        if len(pk_obs) > 0:\n",
    "            obs_times.update(pk_obs['TIME'].values)\n",
    "        if len(pd_obs) > 0:\n",
    "            obs_times.update(pd_obs['TIME'].values)\n",
    "        \n",
    "        if not obs_times:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        sim_times = torch.tensor(sorted(obs_times), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        baseline_biomarker = self.get_baseline_biomarker(subject_data['id'])\n",
    "        \n",
    "        # Simulate with discrete dosing and get parameters used\n",
    "        try:\n",
    "            solution, pk_params, pd_params = self.model.simulate_subject_discrete(\n",
    "                subject_id, covariates, dose_schedule, sim_times, baseline_biomarker)\n",
    "            \n",
    "            # Check for numerical issues\n",
    "            if torch.isnan(solution).any() or torch.isinf(solution).any():\n",
    "                print(f\"Warning: Numerical instability in subject {subject_id}\")\n",
    "                return torch.tensor(100.0, device=self.device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Simulation failed for subject {subject_id}: {e}\")\n",
    "            return torch.tensor(100.0, device=self.device)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        n_terms = 0\n",
    "        \n",
    "        # PK loss (log scale) - if PK data available\n",
    "        if len(pk_obs) > 0:\n",
    "            # Use the same Vc that was used in simulation\n",
    "            Vc = pk_params[2]\n",
    "            \n",
    "            pk_times_tensor = torch.tensor(pk_obs['TIME'].values, dtype=torch.float32, device=self.device)\n",
    "            pk_obs_values = torch.tensor(pk_obs['DV'].values, dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            # Convert amounts to concentrations using simulation parameters\n",
    "            central_amounts = solution[:, 1]\n",
    "            concentrations = central_amounts / Vc\n",
    "            \n",
    "            # Interpolate to observation times\n",
    "            pred_concentrations = torch_interp(pk_times_tensor, sim_times, concentrations)\n",
    "            \n",
    "            # Log-scale loss with numerical stability\n",
    "            pk_loss = nn.MSELoss()(\n",
    "                torch.log(pred_concentrations + 0.001),\n",
    "                torch.log(pk_obs_values + 0.001)\n",
    "            )\n",
    "            \n",
    "            total_loss += pk_loss\n",
    "            n_terms += 1\n",
    "        \n",
    "        # PD loss (linear scale) - if PD data available\n",
    "        if len(pd_obs) > 0:\n",
    "            pd_times_tensor = torch.tensor(pd_obs['TIME'].values, dtype=torch.float32, device=self.device)\n",
    "            pd_obs_values = torch.tensor(pd_obs['DV'].values, dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            # Extract biomarker predictions\n",
    "            biomarker_levels = solution[:, 3]\n",
    "            pred_biomarkers = torch_interp(pd_times_tensor, sim_times, biomarker_levels)\n",
    "            \n",
    "            # Linear-scale loss\n",
    "            pd_loss = nn.MSELoss()(pred_biomarkers, pd_obs_values)\n",
    "            total_loss += pd_loss\n",
    "            n_terms += 1\n",
    "        \n",
    "        return total_loss / max(n_terms, 1)\n",
    "    \n",
    "    def train_epoch(self, subject_list):\n",
    "        \"\"\"Train for one epoch with discrete dosing\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_subjects = 0\n",
    "        \n",
    "        subjects = subject_list.copy()\n",
    "        np.random.shuffle(subjects)\n",
    "        \n",
    "        for subject_id in subjects:\n",
    "            try:\n",
    "                subject_data = self.data_processor.get_subject_data(subject_id)\n",
    "                print(f\"Training on subject {subject_id}...\")\n",
    "                loss = self.compute_loss_discrete(subject_data)\n",
    "                \n",
    "                if loss.item() > 0 and loss.item() < 50:  # Valid loss range\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += loss.item()\n",
    "                    n_subjects += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Training failed for subject {subject_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return epoch_loss / max(n_subjects, 1)\n",
    "    \n",
    "    def validate_epoch(self, subject_list):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        n_subjects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for subject_id in subject_list:\n",
    "                try:\n",
    "                    subject_data = self.data_processor.get_subject_data(subject_id)\n",
    "                    loss = self.compute_loss_discrete(subject_data)\n",
    "                    \n",
    "                    if loss.item() > 0 and loss.item() < 50:\n",
    "                        val_loss += loss.item()\n",
    "                        n_subjects += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        return val_loss / max(n_subjects, 1)\n",
    "    \n",
    "    def train_discrete(self, epochs=50, print_every=1):\n",
    "        \"\"\"Main training loop for discrete dosing model\"\"\"\n",
    "        \n",
    "        print(f\"\\nStarting discrete dosing training for {epochs} epochs...\")\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss = self.train_epoch(self.data_processor.train_subjects)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = self.validate_epoch(self.data_processor.val_subjects)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Track losses\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_discrete_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                print(f\"  Epoch {epoch+1:3d}: Train={train_loss:.4f}, Val={val_loss:.4f}, LR={lr:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= 15:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_discrete_model.pth'))\n",
    "        print(f\"Discrete dosing training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return best_val_loss\n",
    "    \n",
    "    def validate_model_predictions(self, n_test_subjects=10):\n",
    "        \"\"\"Validate model predictions against observed data\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL VALIDATION: PREDICTIONS vs OBSERVATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        pk_predictions, pk_observations = [], []\n",
    "        pd_predictions, pd_observations = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Test on validation subjects\n",
    "            test_subjects = self.data_processor.val_subjects[:n_test_subjects]\n",
    "            \n",
    "            for subject_id in test_subjects:\n",
    "                try:\n",
    "                    subject_data = self.data_processor.get_subject_data(subject_id)\n",
    "                    \n",
    "                    # Skip subjects with no dosing data\n",
    "                    if len(subject_data['dosing_data']) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    covariates = subject_data['covariates']\n",
    "                    dose_schedule = [(row['TIME'], row['AMT']) \n",
    "                                for _, row in subject_data['dosing_data'].iterrows()]\n",
    "                    \n",
    "                    # Get all observation times\n",
    "                    obs_times = set()\n",
    "                    if len(subject_data['pk_data']) > 0:\n",
    "                        obs_times.update(subject_data['pk_data']['TIME'].values)\n",
    "                    if len(subject_data['pd_data']) > 0:\n",
    "                        obs_times.update(subject_data['pd_data']['TIME'].values)\n",
    "                    \n",
    "                    if not obs_times:\n",
    "                        continue\n",
    "                    \n",
    "                    sim_times = torch.tensor(sorted(obs_times), dtype=torch.float32)\n",
    "\n",
    "                    baseline_biomarker = self.get_baseline_biomarker(subject_id)\n",
    "                    \n",
    "                    # Simulate\n",
    "                    solution, pk_params, pd_params = self.model.simulate_subject_discrete(\n",
    "                        subject_id, covariates, dose_schedule, sim_times, baseline_biomarker)\n",
    "                    \n",
    "                    # PK validation\n",
    "                    if len(subject_data['pk_data']) > 0:\n",
    "                        # Get predicted concentrations\n",
    "                        total_dose = sum(amt for _, amt in dose_schedule)\n",
    "                        dose_intensity = total_dose / covariates[0]\n",
    "                        pk_params, _ = self.model.param_estimator.predict_single_subject(\n",
    "                            subject_id, covariates[0], covariates[1], dose_intensity)\n",
    "                        Vc = pk_params[2]\n",
    "                        \n",
    "                        pk_times = torch.tensor(subject_data['pk_data']['TIME'].values, device=self.device)\n",
    "                        concentrations = solution[:, 1] / Vc\n",
    "                        pred_conc = torch_interp(pk_times, sim_times, concentrations)\n",
    "                        \n",
    "                        pk_predictions.extend(pred_conc.numpy())\n",
    "                        pk_observations.extend(subject_data['pk_data']['DV'].values)\n",
    "                    \n",
    "                    # PD validation\n",
    "                    if len(subject_data['pd_data']) > 0:\n",
    "                        pd_times = torch.tensor(subject_data['pd_data']['TIME'].values, device=self.device)\n",
    "                        biomarkers = solution[:, 3]\n",
    "                        pred_biomarker = torch_interp(pd_times, sim_times, biomarkers)\n",
    "                        \n",
    "                        pd_predictions.extend(pred_biomarker.numpy())\n",
    "                        pd_observations.extend(subject_data['pd_data']['DV'].values)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Validation failed for subject {subject_id}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # Create validation plots\n",
    "        self.plot_predictions_vs_observations(pk_predictions, pk_observations, \n",
    "                                            pd_predictions, pd_observations)\n",
    "        \n",
    "        return pk_predictions, pk_observations, pd_predictions, pd_observations\n",
    "    \n",
    "    def plot_predictions_vs_observations(self, pk_pred, pk_obs, pd_pred, pd_obs):\n",
    "        \"\"\"Plot predicted vs observed concentrations and biomarkers\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        if pk_pred and pk_obs:\n",
    "            from sklearn.metrics import r2_score\n",
    "            \n",
    "                    # Remove NaN values first\n",
    "            pk_pred = np.array(pk_pred)\n",
    "            pk_obs = np.array(pk_obs)\n",
    "            valid_mask = np.isfinite(pk_pred) & np.isfinite(pk_obs) & (pk_pred > 0) & (pk_obs > 0)\n",
    "            pk_pred_clean = pk_pred[valid_mask]\n",
    "            pk_obs_clean = pk_obs[valid_mask]\n",
    "            \n",
    "            if len(pk_pred_clean) > 0:\n",
    "                # PK: Log-scale R² (matching log-scale loss)\n",
    "                log_pred = np.log(pk_pred_clean + 0.001)\n",
    "                log_obs = np.log(pk_obs_clean + 0.001)\n",
    "                pk_r2 = r2_score(log_obs, log_pred)\n",
    "                \n",
    "                # Plot on log scale\n",
    "                axes[0, 0].scatter(pk_obs_clean, pk_pred_clean, alpha=0.6, color='blue')\n",
    "                min_pk, max_pk = min(pk_obs_clean), max(pk_obs_clean)\n",
    "                axes[0, 0].plot([min_pk, max_pk], [min_pk, max_pk], 'r--', alpha=0.8)\n",
    "                axes[0, 0].set_xlabel('Observed Concentration (ng/mL)')\n",
    "                axes[0, 0].set_ylabel('Predicted Concentration (ng/mL)')\n",
    "                axes[0, 0].set_title(f'PK Model: Log-scale R² = {pk_r2:.3f}')\n",
    "                axes[0, 0].set_xscale('log')\n",
    "                axes[0, 0].set_yscale('log')\n",
    "                axes[0, 0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # PK residuals (log scale)\n",
    "                log_residuals = log_pred - log_obs\n",
    "                axes[1, 0].scatter(pk_obs_clean, log_residuals, alpha=0.6, color='blue')\n",
    "                axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "                axes[1, 0].set_xlabel('Observed Concentration (ng/mL)')\n",
    "                axes[1, 0].set_ylabel('Log Residuals')\n",
    "                axes[1, 0].set_title('PK Residuals (Log Scale)')\n",
    "                axes[1, 0].set_xscale('log')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        if pd_pred and pd_obs:\n",
    "            # PD predictions vs observations\n",
    "            axes[0, 1].scatter(pd_obs, pd_pred, alpha=0.6, color='green')\n",
    "            min_pd, max_pd = min(pd_obs), max(pd_obs)\n",
    "            axes[0, 1].plot([min_pd, max_pd], [min_pd, max_pd], 'r--', alpha=0.8)\n",
    "            axes[0, 1].axhline(y=3.3, color='red', linestyle=':', alpha=0.7, label='Target threshold')\n",
    "            axes[0, 1].axvline(x=3.3, color='red', linestyle=':', alpha=0.7)\n",
    "            \n",
    "            pd_r2 = r2_score(pd_obs, pd_pred)\n",
    "            \n",
    "            axes[0, 1].set_xlabel('Observed Biomarker (ng/mL)')\n",
    "            axes[0, 1].set_ylabel('Predicted Biomarker (ng/mL)')\n",
    "            axes[0, 1].set_title(f'PD Model Validation (R² = {pd_r2:.3f})')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # PD residuals\n",
    "            residuals = np.array(pd_pred) - np.array(pd_obs)\n",
    "            axes[1, 1].scatter(pd_obs, residuals, alpha=0.6, color='green')\n",
    "            axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "            axes[1, 1].set_xlabel('Observed Biomarker (ng/mL)')\n",
    "            axes[1, 1].set_ylabel('Residuals (ng/mL)')\n",
    "            axes[1, 1].set_title('PD Residuals')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e6c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: DOSE OPTIMIZATION WITH DISCRETE DOSING\n",
    "# ============================================================================\n",
    "\n",
    "class DiscreteDoseOptimizer:\n",
    "    \"\"\"Dose optimization using the stable discrete dosing model\"\"\"\n",
    "    \n",
    "    def __init__(self, trained_model, trainer, device=device):\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"SECTION 5: DOSE OPTIMIZATION - DISCRETE DOSING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.model = trained_model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        # Extract baseline distribution from training data\n",
    "        self.baseline_distribution = trainer.extract_baseline_distribution()\n",
    "        \n",
    "        print(\"Discrete Dose Optimization Setup:\")\n",
    "        print(\"  Target: 90% of population with biomarker < 3.3 ng/mL\")\n",
    "        print(\"  Success criteria: 95% of steady-state time below threshold\")\n",
    "        print(\"  Steady-state: Last 168 hours of simulation\")\n",
    "        print(\"  Method: Discrete dosing (no numerical spikes)\")\n",
    "    \n",
    "    def simulate_population_response(self, dose_mg, dosing_interval_h, population_params,\n",
    "                               duration_h=672, target_threshold=3.3):\n",
    "        \"\"\"Simulate with realistic baseline distribution\"\"\"\n",
    "        \n",
    "        success_count = 0\n",
    "        total_subjects = len(population_params)\n",
    "        \n",
    "        # Sample baselines from actual distribution\n",
    "        if hasattr(self, 'baseline_distribution') and len(self.baseline_distribution) > 0:\n",
    "            virtual_baselines = np.random.choice(self.baseline_distribution, size=total_subjects)\n",
    "        else:\n",
    "            # Fallback if no distribution available\n",
    "            print(\"Warning: No baseline distribution available for optimization, using default 16.0 ng/mL\")\n",
    "            virtual_baselines = np.full(total_subjects, 16.0)\n",
    "        \n",
    "        for i, (bw, comed) in enumerate(population_params):\n",
    "            baseline_biomarker = float(virtual_baselines[i])\n",
    "            \n",
    "            # Create discrete dose schedule\n",
    "            dose_times = list(range(0, int(duration_h), int(dosing_interval_h)))\n",
    "            dose_schedule = [(float(t), float(dose_mg)) for t in dose_times]\n",
    "            \n",
    "            covariates = torch.tensor([float(bw), float(comed)], dtype=torch.float32, device=self.device)\n",
    "            sim_times = torch.arange(0, duration_h, 6, dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            try:\n",
    "                embedding_idx = i % self.model.param_estimator.n_subjects\n",
    "                \n",
    "                # Use sampled baseline\n",
    "                solution = self.model.simulate_subject_discrete(\n",
    "                    embedding_idx, covariates, dose_schedule, sim_times, baseline_biomarker)\n",
    "                \n",
    "                # Check steady-state suppression\n",
    "                steady_start = int(duration_h * 0.75)\n",
    "                steady_mask = sim_times >= steady_start\n",
    "                steady_biomarkers = solution[steady_mask, 3]\n",
    "                \n",
    "                if len(steady_biomarkers) > 0:\n",
    "                    suppression_fraction = (steady_biomarkers < target_threshold).float().mean()\n",
    "                    \n",
    "                    if suppression_fraction >= 0.95:\n",
    "                        success_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return success_count / total_subjects if total_subjects > 0 else 0.0\n",
    "    \n",
    "    def optimize_dose_discrete(self, dosing_interval_h, population_params, \n",
    "                             target_success_rate=0.90, dose_increment=0.5):\n",
    "        \"\"\"Find optimal dose using binary search with discrete dosing\"\"\"\n",
    "        \n",
    "        interval_name = f\"{dosing_interval_h}h\" if dosing_interval_h < 168 else f\"{dosing_interval_h//24}d\"\n",
    "        print(f\"\\nOptimizing {interval_name} discrete dosing...\")\n",
    "        print(f\"  Target success rate: {target_success_rate*100}%\")\n",
    "        print(f\"  Population size: {len(population_params)}\")\n",
    "        \n",
    "        # Binary search for optimal dose\n",
    "        low_dose, high_dose = dose_increment, 50.0  # Reasonable upper bound\n",
    "        \n",
    "        # Check if max dose achieves target\n",
    "        max_success = self.simulate_population_response(\n",
    "            high_dose, dosing_interval_h, population_params[:100])  # Sample for speed\n",
    "        \n",
    "        if max_success < target_success_rate:\n",
    "            print(f\"  Warning: Even {high_dose}mg only achieves {max_success*100:.1f}% success\")\n",
    "            return high_dose\n",
    "        \n",
    "        # Binary search\n",
    "        tolerance = dose_increment / 2\n",
    "        while high_dose - low_dose > tolerance:\n",
    "            mid_dose = (low_dose + high_dose) / 2\n",
    "            mid_dose = round(mid_dose / dose_increment) * dose_increment\n",
    "            \n",
    "            success_rate = self.simulate_population_response(\n",
    "                mid_dose, dosing_interval_h, population_params[:200])\n",
    "            \n",
    "            print(f\"    Dose {mid_dose:5.1f} mg: {success_rate*100:5.1f}% success\")\n",
    "            \n",
    "            if success_rate >= target_success_rate:\n",
    "                high_dose = mid_dose\n",
    "            else:\n",
    "                low_dose = mid_dose\n",
    "        \n",
    "        optimal_dose = high_dose\n",
    "        print(f\"  → Optimal discrete dose: {optimal_dose} mg\")\n",
    "        \n",
    "        return optimal_dose\n",
    "    \n",
    "    def answer_discrete_questions(self):\n",
    "        \"\"\"Answer all 5 questions using discrete dosing approach\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANSWERING THE 5 QUESTIONS - DISCRETE DOSING METHOD\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Generate populations (same as original but smaller for testing)\n",
    "        np.random.seed(42)\n",
    "        n_pop = 500  # Reduced for faster computation\n",
    "        \n",
    "        # Base population\n",
    "        base_population = []\n",
    "        for _ in range(n_pop):\n",
    "            bw = np.random.uniform(51, 100)\n",
    "            comed = np.random.choice([0, 1])\n",
    "            base_population.append((bw, comed))\n",
    "        \n",
    "        # Extended BW population\n",
    "        extended_population = []\n",
    "        for _ in range(n_pop):\n",
    "            bw = np.random.uniform(70, 140)\n",
    "            comed = np.random.choice([0, 1])\n",
    "            extended_population.append((bw, comed))\n",
    "        \n",
    "        # No COMED population\n",
    "        no_comed_population = []\n",
    "        for _ in range(n_pop):\n",
    "            bw = np.random.uniform(51, 100)\n",
    "            comed = 0\n",
    "            no_comed_population.append((bw, comed))\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Question 1: Daily dosing (base population)\n",
    "        print(\"\\nQUESTION 1: Optimal daily dose (base population, 90% target)\")\n",
    "        daily_dose_base = self.optimize_dose_discrete(24, base_population, 0.90, 0.5)\n",
    "        results['Q1_daily_base_90'] = daily_dose_base\n",
    "        \n",
    "        # Question 2: Weekly dosing (base population)\n",
    "        print(\"\\nQUESTION 2: Optimal weekly dose (base population, 90% target)\")\n",
    "        weekly_dose_base = self.optimize_dose_discrete(168, base_population, 0.90, 5.0)\n",
    "        results['Q2_weekly_base_90'] = weekly_dose_base\n",
    "        \n",
    "        # Question 3: Extended BW population\n",
    "        print(\"\\nQUESTION 3: Extended BW population (70-140 kg, 90% target)\")\n",
    "        daily_dose_ext = self.optimize_dose_discrete(24, extended_population, 0.90, 0.5)\n",
    "        weekly_dose_ext = self.optimize_dose_discrete(168, extended_population, 0.90, 5.0)\n",
    "        results['Q3_daily_ext_90'] = daily_dose_ext\n",
    "        results['Q3_weekly_ext_90'] = weekly_dose_ext\n",
    "        \n",
    "        # Question 4: No concomitant medication\n",
    "        print(\"\\nQUESTION 4: No concomitant medication allowed (90% target)\")\n",
    "        daily_dose_no_comed = self.optimize_dose_discrete(24, no_comed_population, 0.90, 0.5)\n",
    "        weekly_dose_no_comed = self.optimize_dose_discrete(168, no_comed_population, 0.90, 5.0)\n",
    "        results['Q4_daily_no_comed_90'] = daily_dose_no_comed\n",
    "        results['Q4_weekly_no_comed_90'] = weekly_dose_no_comed\n",
    "        \n",
    "        # Question 5: 75% success rate scenarios\n",
    "        print(\"\\nQUESTION 5: 75% success rate scenarios\")\n",
    "        \n",
    "        print(\"  Base population (75% target):\")\n",
    "        daily_dose_base_75 = self.optimize_dose_discrete(24, base_population, 0.75, 0.5)\n",
    "        weekly_dose_base_75 = self.optimize_dose_discrete(168, base_population, 0.75, 5.0)\n",
    "        results['Q5_daily_base_75'] = daily_dose_base_75\n",
    "        results['Q5_weekly_base_75'] = weekly_dose_base_75\n",
    "        \n",
    "        print(\"  Extended BW population (75% target):\")\n",
    "        daily_dose_ext_75 = self.optimize_dose_discrete(24, extended_population, 0.75, 0.5)\n",
    "        weekly_dose_ext_75 = self.optimize_dose_discrete(168, extended_population, 0.75, 5.0)\n",
    "        results['Q5_daily_ext_75'] = daily_dose_ext_75\n",
    "        results['Q5_weekly_ext_75'] = weekly_dose_ext_75\n",
    "        \n",
    "        print(\"  No COMED population (75% target):\")\n",
    "        daily_dose_no_comed_75 = self.optimize_dose_discrete(24, no_comed_population, 0.75, 0.5)\n",
    "        weekly_dose_no_comed_75 = self.optimize_dose_discrete(168, no_comed_population, 0.75, 5.0)\n",
    "        results['Q5_daily_no_comed_75'] = daily_dose_no_comed_75\n",
    "        results['Q5_weekly_no_comed_75'] = weekly_dose_no_comed_75\n",
    "        \n",
    "        # Print comprehensive results\n",
    "        self.print_discrete_results(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_discrete_results(self, results):\n",
    "        \"\"\"Print comprehensive results from discrete dosing optimization\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DISCRETE DOSING OPTIMIZATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"{'Scenario':<35} {'Daily (mg)':<12} {'Weekly (mg)':<12} {'Success Rate'}\")\n",
    "        print(f\"{'-'*35} {'-'*12} {'-'*12} {'-'*12}\")\n",
    "        \n",
    "        # 90% success rate scenarios\n",
    "        print(f\"{'Base population (90%)':<35} {results['Q1_daily_base_90']:<12} {results['Q2_weekly_base_90']:<12} {'90%'}\")\n",
    "        print(f\"{'Extended BW 70-140kg (90%)':<35} {results['Q3_daily_ext_90']:<12} {results['Q3_weekly_ext_90']:<12} {'90%'}\")\n",
    "        print(f\"{'No COMED allowed (90%)':<35} {results['Q4_daily_no_comed_90']:<12} {results['Q4_weekly_no_comed_90']:<12} {'90%'}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # 75% success rate scenarios  \n",
    "        print(f\"{'Base population (75%)':<35} {results['Q5_daily_base_75']:<12} {results['Q5_weekly_base_75']:<12} {'75%'}\")\n",
    "        print(f\"{'Extended BW 70-140kg (75%)':<35} {results['Q5_daily_ext_75']:<12} {results['Q5_weekly_ext_75']:<12} {'75%'}\")\n",
    "        print(f\"{'No COMED allowed (75%)':<35} {results['Q5_daily_no_comed_75']:<12} {results['Q5_weekly_no_comed_75']:<12} {'75%'}\")\n",
    "        \n",
    "        # Analysis\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"DISCRETE DOSING INSIGHTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        weekly_daily_ratio = results['Q2_weekly_base_90'] / (results['Q1_daily_base_90'] * 7)\n",
    "        dose_reduction_75vs90 = (results['Q1_daily_base_90'] - results['Q5_daily_base_75']) / results['Q1_daily_base_90'] * 100\n",
    "        \n",
    "        print(f\"1. Weekly vs Daily Dosing Efficiency:\")\n",
    "        print(f\"   Weekly dose: {results['Q2_weekly_base_90']} mg\")\n",
    "        print(f\"   Daily dose × 7: {results['Q1_daily_base_90'] * 7} mg\")\n",
    "        print(f\"   Weekly efficiency: {weekly_daily_ratio:.2f} (1.0 = same total dose)\")\n",
    "        \n",
    "        print(f\"\\n2. Population Effects on Dosing:\")\n",
    "        bw_effect = (results['Q3_daily_ext_90'] - results['Q1_daily_base_90']) / results['Q1_daily_base_90'] * 100\n",
    "        comed_effect = (results['Q1_daily_base_90'] - results['Q4_daily_no_comed_90']) / results['Q1_daily_base_90'] * 100\n",
    "        print(f\"   Extended BW impact: {bw_effect:+.0f}% dose change\")\n",
    "        print(f\"   COMED restriction benefit: {comed_effect:+.0f}% dose reduction\")\n",
    "        \n",
    "        print(f\"\\n3. Success Rate Flexibility:\")\n",
    "        print(f\"   Dose reduction (90%→75%): {dose_reduction_75vs90:.0f}%\")\n",
    "        \n",
    "        print(f\"\\n4. Clinical Translation:\")\n",
    "        print(f\"   Study max dose tested: 10 mg daily\")\n",
    "        print(f\"   Recommended optimal dose: {results['Q1_daily_base_90']} mg daily\")\n",
    "        print(f\"   Extrapolation factor: {results['Q1_daily_base_90'] / 10:.1f}×\")\n",
    "        \n",
    "        print(f\"\\n5. Discrete Dosing Advantages:\")\n",
    "        print(f\"   ✓ Numerically stable (no dose rate spikes)\")\n",
    "        print(f\"   ✓ Clinically realistic (discrete administrations)\")\n",
    "        print(f\"   ✓ Computationally reliable\")\n",
    "        print(f\"   ✓ Suitable for regulatory submissions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56f5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION PIPELINE - DISCRETE DOSING VERSION\n",
    "# ============================================================================\n",
    "\n",
    "def main_discrete():\n",
    "    \"\"\"Execute the complete discrete dosing analysis pipeline\"\"\"\n",
    "    \n",
    "    print(\"DISCRETE DOSING PK-PD MODEL WITH NEURAL PARAMETER ESTIMATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Objective: Stable dose optimization using discrete dosing approach\")\n",
    "    print(\"Target: 90% population biomarker suppression < 3.3 ng/mL\")\n",
    "    print(\"Method: Discrete doses eliminate numerical instability\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Section 1: Data Preparation\n",
    "        data_processor = DataProcessor('data/EstData.csv')\n",
    "        data_processor.load_and_examine_data()\n",
    "        data_processor.quality_checks()\n",
    "        data_processor.create_subject_summaries()\n",
    "        data_processor.train_validation_split()\n",
    "        \n",
    "        # Section 2: Neural Network Architecture\n",
    "        n_subjects = len(data_processor.subject_info)\n",
    "        param_estimator = DiscreteNeuralParameterEstimator(n_subjects, embedding_dim=16, device=device)\n",
    "        param_estimator.to(device)\n",
    "        \n",
    "        # Section 3: Discrete Dosing Mechanistic Model\n",
    "        pkpd_model = DiscreteDosePKPDModel(param_estimator, device=device)\n",
    "        pkpd_model.to(device)\n",
    "        \n",
    "        # Section 4: Training\n",
    "        trainer = DiscretePKPDTrainer(pkpd_model, data_processor)\n",
    "        best_loss = trainer.train_discrete(epochs=100, print_every=1)\n",
    "        \n",
    "        if best_loss < 10:  # Only proceed if training was successful\n",
    "            print(f\"✓ Training successful (final loss: {best_loss:.3f})\")\n",
    "        else:\n",
    "            print(f\"✗ Training failed (loss too high: {best_loss:.3f})\")\n",
    "        # Validate model predictions\n",
    "        trainer.validate_model_predictions(n_test_subjects=15)\n",
    "        \n",
    "        # Section 5: Dose Optimization\n",
    "        optimizer = DiscreteDoseOptimizer(pkpd_model, trainer)\n",
    "        results = optimizer.answer_discrete_questions()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DISCRETE DOSING ANALYSIS COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"✓ Mechanistic PK-PD model successfully trained\")\n",
    "        print(\"✓ Discrete dosing eliminates numerical instability\") \n",
    "        print(\"✓ All 5 questions answered with stable dose recommendations\")\n",
    "        print(\"✓ Results clinically realistic and computationally reliable\")\n",
    "        \n",
    "        return results, trainer, optimizer, pkpd_model, data_processor\n",
    "      \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Discrete dosing analysis failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Quick test function\n",
    "def test_discrete_dosing():\n",
    "    \"\"\"Quick test of discrete dosing approach on single subject\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING DISCRETE DOSING APPROACH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create minimal test setup\n",
    "    param_estimator = DiscreteNeuralParameterEstimator(10, embedding_dim=8, device=device)\n",
    "    param_estimator.to(device)\n",
    "    model = DiscreteDosePKPDModel(param_estimator, device=device)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Test discrete dosing simulation\n",
    "    subject_id = 0\n",
    "    covariates = torch.tensor([70.0, 0.0], device=device)  # 70kg, no COMED\n",
    "    dose_schedule = [(0.0, 3.0), (24.0, 3.0), (48.0, 3.0)]  # 3mg daily × 3 days\n",
    "    sim_times = torch.linspace(0, 72, 25, device=device)  # 3 days, 25 points\n",
    "    \n",
    "    try:\n",
    "        solution, pk_params, pd_params = model.simulate_subject_discrete(\n",
    "            subject_id, covariates, dose_schedule, sim_times)\n",
    "        \n",
    "        print(f\"✓ Discrete dosing test successful!\")\n",
    "        print(f\"  Solution shape: {solution.shape}\")\n",
    "        print(f\"  Final biomarker: {solution[-1, 3]:.3f} ng/mL\")\n",
    "        print(f\"  No NaN values: {not torch.isnan(solution).any()}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Discrete dosing test failed: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80cde95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING DISCRETE DOSING APPROACH\n",
      "============================================================\n",
      "Neural Network Architecture:\n",
      "  Subjects: 10\n",
      "  Embedding dimension: 8\n",
      "  Network: 3  → 9\n",
      "  Total parameters: 135\n",
      "================================================================================\n",
      "SECTION 3: DISCRETE DOSING MECHANISTIC MODEL\n",
      "================================================================================\n",
      "Mechanistic Model Components:\n",
      "  PK Model: Two-compartment with first-order absorption\n",
      "  PD Model: Indirect response (inhibition of production)\n",
      "  Dosing: Discrete doses added directly to depot compartment\n",
      "  NO continuous dose rates or numerical spikes\n",
      "✓ Discrete dosing test successful!\n",
      "  Solution shape: torch.Size([25, 4])\n",
      "  Final biomarker: 10.100 ng/mL\n",
      "  No NaN values: True\n",
      "\n",
      "============================================================\n",
      "PROCEEDING TO FULL DISCRETE DOSING ANALYSIS\n",
      "============================================================\n",
      "DISCRETE DOSING PK-PD MODEL WITH NEURAL PARAMETER ESTIMATION\n",
      "================================================================================\n",
      "Objective: Stable dose optimization using discrete dosing approach\n",
      "Target: 90% population biomarker suppression < 3.3 ng/mL\n",
      "Method: Discrete doses eliminate numerical instability\n",
      "\n",
      "================================================================================\n",
      "SECTION 1: DATA PREPARATION - DISCRETE DOSING VERSION\n",
      "================================================================================\n",
      "Loading data from data/EstData.csv...\n",
      "\n",
      "Data Overview:\n",
      "  Total records: 2820\n",
      "  Subjects: 48\n",
      "  Columns: ['ID', 'BW', 'COMED', 'DOSE', 'TIME', 'DV', 'EVID', 'MDV', 'AMT', 'CMT', 'DVID']\n",
      "\n",
      "Data Breakdown:\n",
      "  PK observations (DVID=1): 864\n",
      "  PD observations (DVID=2): 1200\n",
      "  Dosing events (EVID=1): 756\n",
      "\n",
      "Data Quality Checks:\n",
      "  ✓ No missing values\n",
      "  Dose distribution: {0: 300, 1: 840, 3: 840, 10: 840}\n",
      "  Body weight range: 51.0 - 100.0 kg\n",
      "  COMED distribution: {0: 1455, 1: 1365}\n",
      "  Time range: 0 - 1176 hours\n",
      "\n",
      "Creating subject summaries...\n",
      "  Subject summary created for 48 subjects\n",
      "  Dose group sizes: {0: 12, 1: 12, 3: 12, 10: 12}\n",
      "\n",
      "Splitting data (validation fraction: 0.2)...\n",
      "  Training subjects: 40\n",
      "  Validation subjects: 8\n",
      "Neural Network Architecture:\n",
      "  Subjects: 48\n",
      "  Embedding dimension: 16\n",
      "  Network: 3  → 9\n",
      "  Total parameters: 135\n",
      "================================================================================\n",
      "SECTION 3: DISCRETE DOSING MECHANISTIC MODEL\n",
      "================================================================================\n",
      "Mechanistic Model Components:\n",
      "  PK Model: Two-compartment with first-order absorption\n",
      "  PD Model: Indirect response (inhibition of production)\n",
      "  Dosing: Discrete doses added directly to depot compartment\n",
      "  NO continuous dose rates or numerical spikes\n",
      "================================================================================\n",
      "SECTION 4: DISCRETE DOSING NEURAL NETWORK TRAINING\n",
      "================================================================================\n",
      "Computing baseline for subject 1  (not cached)\n",
      "Computing baseline for subject 2  (not cached)\n",
      "Computing baseline for subject 3  (not cached)\n",
      "Computing baseline for subject 4  (not cached)\n",
      "Computing baseline for subject 5  (not cached)\n",
      "Computing baseline for subject 6  (not cached)\n",
      "Computing baseline for subject 7  (not cached)\n",
      "Computing baseline for subject 8  (not cached)\n",
      "Computing baseline for subject 9  (not cached)\n",
      "Computing baseline for subject 10  (not cached)\n",
      "Computing baseline for subject 11  (not cached)\n",
      "Computing baseline for subject 12  (not cached)\n",
      "Computing baseline for subject 13  (not cached)\n",
      "Computing baseline for subject 14  (not cached)\n",
      "Computing baseline for subject 15  (not cached)\n",
      "Computing baseline for subject 16  (not cached)\n",
      "Computing baseline for subject 17  (not cached)\n",
      "Computing baseline for subject 18  (not cached)\n",
      "Computing baseline for subject 19  (not cached)\n",
      "Computing baseline for subject 20  (not cached)\n",
      "Computing baseline for subject 21  (not cached)\n",
      "Computing baseline for subject 22  (not cached)\n",
      "Computing baseline for subject 23  (not cached)\n",
      "Computing baseline for subject 24  (not cached)\n",
      "Computing baseline for subject 25  (not cached)\n",
      "Computing baseline for subject 26  (not cached)\n",
      "Computing baseline for subject 27  (not cached)\n",
      "Computing baseline for subject 28  (not cached)\n",
      "Computing baseline for subject 29  (not cached)\n",
      "Computing baseline for subject 30  (not cached)\n",
      "Computing baseline for subject 31  (not cached)\n",
      "Computing baseline for subject 32  (not cached)\n",
      "Computing baseline for subject 33  (not cached)\n",
      "Computing baseline for subject 34  (not cached)\n",
      "Computing baseline for subject 35  (not cached)\n",
      "Computing baseline for subject 36  (not cached)\n",
      "Computing baseline for subject 37  (not cached)\n",
      "Computing baseline for subject 38  (not cached)\n",
      "Computing baseline for subject 39  (not cached)\n",
      "Computing baseline for subject 40  (not cached)\n",
      "Computing baseline for subject 41  (not cached)\n",
      "Computing baseline for subject 42  (not cached)\n",
      "Computing baseline for subject 43  (not cached)\n",
      "Computing baseline for subject 44  (not cached)\n",
      "Computing baseline for subject 45  (not cached)\n",
      "Computing baseline for subject 46  (not cached)\n",
      "Computing baseline for subject 47  (not cached)\n",
      "Computing baseline for subject 48  (not cached)\n",
      "Training Setup:\n",
      "  Device: cpu\n",
      "  Optimizer: Adam (lr=1e-3, weight_decay=1e-4)\n",
      "  Scheduler: StepLR (step_size=20, gamma=0.7)\n",
      "  Model parameters: 135\n",
      "\n",
      "Starting discrete dosing training for 100 epochs...\n",
      "Training on subject 5...\n",
      "Training on subject 43...\n",
      "Training on subject 13...\n",
      "Training on subject 1...\n",
      "Training on subject 44...\n",
      "Training on subject 25...\n",
      "Training on subject 36...\n",
      "Training on subject 33...\n",
      "Training on subject 18...\n",
      "Training on subject 35...\n",
      "Training on subject 47...\n",
      "Training on subject 28...\n",
      "Training on subject 21...\n",
      "Training on subject 15...\n",
      "Training on subject 26...\n",
      "Training on subject 41...\n",
      "Training on subject 12...\n",
      "Training on subject 6...\n",
      "Training on subject 42...\n",
      "Training on subject 27...\n",
      "Training on subject 37...\n",
      "Training on subject 20...\n",
      "Training on subject 48...\n",
      "Training on subject 19...\n",
      "Training on subject 45...\n",
      "Training on subject 3...\n",
      "Training on subject 39...\n",
      "Training on subject 8...\n",
      "Training on subject 14...\n",
      "Training on subject 7...\n",
      "Training on subject 40...\n",
      "Training on subject 17...\n",
      "Training on subject 34...\n",
      "Training on subject 24...\n",
      "Training on subject 2...\n",
      "Training on subject 31...\n",
      "Training on subject 9...\n",
      "Training on subject 16...\n",
      "Training on subject 30...\n",
      "Training on subject 4...\n",
      "  Epoch   1: Train=21.5870, Val=16.0871, LR=0.001000\n",
      "Training on subject 6...\n",
      "Training on subject 40...\n",
      "Training on subject 45...\n",
      "Training on subject 36...\n",
      "Training on subject 48...\n",
      "Training on subject 2...\n",
      "Training on subject 4...\n",
      "Training on subject 24...\n",
      "Training on subject 47...\n",
      "Training on subject 39...\n",
      "Training on subject 34...\n",
      "Training on subject 16...\n",
      "Training on subject 41...\n",
      "Training on subject 12...\n",
      "Training on subject 5...\n",
      "Training on subject 27...\n",
      "Training on subject 43...\n",
      "Training on subject 26...\n",
      "Training on subject 3...\n",
      "Training on subject 33...\n",
      "Training on subject 19...\n",
      "Training on subject 21...\n",
      "Training on subject 13...\n",
      "Training on subject 8...\n",
      "Training on subject 14...\n",
      "Training on subject 37...\n",
      "Training on subject 44...\n",
      "Training on subject 9...\n",
      "Training on subject 7...\n",
      "Training on subject 42...\n",
      "Training on subject 1...\n",
      "Training on subject 15...\n",
      "Training on subject 17...\n",
      "Training on subject 35...\n",
      "Training on subject 30...\n",
      "Training on subject 31...\n",
      "Training on subject 28...\n",
      "Training on subject 20...\n",
      "Training on subject 18...\n",
      "Training on subject 25...\n",
      "  Epoch   2: Train=16.4365, Val=13.5006, LR=0.001000\n",
      "Training on subject 30...\n",
      "Training on subject 47...\n",
      "Training on subject 34...\n",
      "Training on subject 37...\n",
      "Training on subject 8...\n",
      "Training on subject 2...\n",
      "Training on subject 14...\n",
      "Training on subject 6...\n",
      "Training on subject 39...\n",
      "Training on subject 25...\n",
      "Training on subject 20...\n",
      "Training on subject 13...\n",
      "Training on subject 18...\n",
      "Training on subject 24...\n",
      "Training on subject 36...\n",
      "Training on subject 43...\n",
      "Training on subject 4...\n",
      "Training on subject 19...\n",
      "Training on subject 35...\n",
      "Training on subject 21...\n",
      "Training on subject 15...\n",
      "Training on subject 12...\n",
      "Training on subject 48...\n",
      "Training on subject 17...\n",
      "Training on subject 9...\n",
      "Training on subject 7...\n",
      "Training on subject 40...\n",
      "Training on subject 27...\n",
      "Training on subject 31...\n",
      "Training on subject 5...\n",
      "Training on subject 41...\n",
      "Training on subject 3...\n",
      "Training on subject 16...\n",
      "Training on subject 1...\n",
      "Training on subject 45...\n",
      "Training on subject 33...\n",
      "Training on subject 26...\n",
      "Training on subject 42...\n",
      "Training on subject 44...\n",
      "Training on subject 28...\n",
      "  Epoch   3: Train=13.6537, Val=11.3549, LR=0.001000\n",
      "Training on subject 17...\n",
      "Training on subject 19...\n",
      "Training on subject 25...\n",
      "Training on subject 2...\n",
      "Training on subject 16...\n",
      "Training on subject 20...\n",
      "Training on subject 4...\n",
      "Training on subject 18...\n",
      "Training on subject 28...\n",
      "Training on subject 31...\n",
      "Training on subject 42...\n",
      "Training on subject 21...\n",
      "Training on subject 14...\n",
      "Training on subject 12...\n",
      "Training on subject 26...\n",
      "Training on subject 35...\n",
      "Training on subject 5...\n",
      "Training on subject 27...\n",
      "Training on subject 8...\n",
      "Training on subject 48...\n",
      "Training on subject 39...\n",
      "Training on subject 3...\n",
      "Training on subject 9...\n",
      "Training on subject 15...\n",
      "Training on subject 44...\n",
      "Training on subject 41...\n",
      "Training on subject 33...\n",
      "Training on subject 40...\n",
      "Training on subject 37...\n",
      "Training on subject 1...\n",
      "Training on subject 43...\n",
      "Training on subject 7...\n",
      "Training on subject 30...\n",
      "Training on subject 45...\n",
      "Training on subject 13...\n",
      "Training on subject 36...\n",
      "Training on subject 24...\n",
      "Training on subject 47...\n",
      "Training on subject 34...\n",
      "Training on subject 6...\n",
      "  Epoch   4: Train=11.9714, Val=10.4232, LR=0.001000\n",
      "Training on subject 35...\n",
      "Training on subject 16...\n",
      "Training on subject 20...\n",
      "Training on subject 12...\n",
      "Training on subject 7...\n",
      "Training on subject 15...\n",
      "Training on subject 8...\n",
      "Training on subject 28...\n",
      "Training on subject 47...\n",
      "Training on subject 18...\n",
      "Training on subject 17...\n",
      "Training on subject 31...\n",
      "Training on subject 9...\n",
      "Training on subject 36...\n",
      "Training on subject 26...\n",
      "Training on subject 33...\n",
      "Training on subject 24...\n",
      "Training on subject 41...\n",
      "Training on subject 39...\n",
      "Training on subject 30...\n",
      "Training on subject 45...\n",
      "Training on subject 13...\n",
      "Training on subject 4...\n",
      "Training on subject 40...\n",
      "Training on subject 6...\n",
      "Training on subject 14...\n",
      "Training on subject 25...\n",
      "Training on subject 42...\n",
      "Training on subject 21...\n",
      "Training on subject 37...\n",
      "Training on subject 1...\n",
      "Training on subject 5...\n",
      "Training on subject 27...\n",
      "Training on subject 34...\n",
      "Training on subject 43...\n",
      "Training on subject 48...\n",
      "Training on subject 19...\n",
      "Training on subject 44...\n",
      "Training on subject 3...\n",
      "Training on subject 2...\n",
      "  Epoch   5: Train=11.1846, Val=10.0811, LR=0.001000\n",
      "Training on subject 26...\n",
      "Training on subject 17...\n",
      "Training on subject 15...\n",
      "Training on subject 6...\n",
      "Training on subject 48...\n",
      "Training on subject 4...\n",
      "Training on subject 16...\n",
      "Training on subject 8...\n",
      "Training on subject 33...\n",
      "Training on subject 31...\n",
      "Training on subject 25...\n",
      "Training on subject 9...\n",
      "Training on subject 36...\n",
      "Training on subject 43...\n",
      "Training on subject 35...\n",
      "Training on subject 5...\n",
      "Training on subject 14...\n",
      "Training on subject 45...\n",
      "Training on subject 41...\n",
      "Training on subject 2...\n",
      "Training on subject 19...\n",
      "Training on subject 47...\n",
      "Training on subject 28...\n",
      "Training on subject 18...\n",
      "Training on subject 44...\n",
      "Training on subject 3...\n",
      "Training on subject 12...\n",
      "Training on subject 27...\n",
      "Training on subject 42...\n",
      "Training on subject 37...\n",
      "Training on subject 30...\n",
      "Training on subject 34...\n",
      "Training on subject 13...\n",
      "Training on subject 1...\n",
      "Training on subject 20...\n",
      "Training on subject 40...\n",
      "Training on subject 7...\n",
      "Training on subject 39...\n",
      "Training on subject 24...\n",
      "Training on subject 21...\n",
      "  Epoch   6: Train=11.0289, Val=10.2759, LR=0.001000\n",
      "Training on subject 44...\n",
      "Training on subject 34...\n",
      "Training on subject 40...\n",
      "Training on subject 15...\n",
      "Training on subject 5...\n",
      "Training on subject 41...\n",
      "Training on subject 36...\n",
      "Training on subject 12...\n",
      "Training on subject 19...\n",
      "Training on subject 6...\n",
      "Training on subject 21...\n",
      "Training on subject 13...\n",
      "Training on subject 14...\n",
      "Training on subject 17...\n",
      "Training on subject 25...\n",
      "Training on subject 33...\n",
      "Training on subject 7...\n",
      "Training on subject 48...\n",
      "Training on subject 31...\n",
      "Training on subject 37...\n",
      "Training on subject 27...\n",
      "Training on subject 28...\n",
      "Training on subject 1...\n",
      "Training on subject 4...\n",
      "Training on subject 9...\n",
      "Training on subject 18...\n",
      "Training on subject 3...\n",
      "Training on subject 26...\n",
      "Training on subject 2...\n",
      "Training on subject 39...\n",
      "Training on subject 16...\n",
      "Training on subject 20...\n",
      "Training on subject 35...\n",
      "Training on subject 47...\n",
      "Training on subject 30...\n",
      "Training on subject 42...\n",
      "Training on subject 24...\n",
      "Training on subject 45...\n",
      "Training on subject 43...\n",
      "Training on subject 8...\n",
      "  Epoch   7: Train=10.6664, Val=9.5319, LR=0.001000\n",
      "Training on subject 20...\n",
      "Training on subject 24...\n",
      "Training on subject 2...\n",
      "Training on subject 12...\n",
      "Training on subject 27...\n",
      "Training on subject 13...\n",
      "Training on subject 7...\n",
      "Training on subject 43...\n",
      "Training on subject 17...\n",
      "Training on subject 47...\n",
      "Training on subject 26...\n",
      "Training on subject 37...\n",
      "Training on subject 28...\n",
      "Training on subject 1...\n",
      "Training on subject 15...\n",
      "Training on subject 8...\n",
      "Training on subject 39...\n",
      "Training on subject 9...\n",
      "Training on subject 31...\n",
      "Training on subject 6...\n",
      "Training on subject 44...\n",
      "Training on subject 16...\n",
      "Training on subject 14...\n",
      "Training on subject 30...\n",
      "Training on subject 19...\n",
      "Training on subject 3...\n",
      "Training on subject 4...\n",
      "Training on subject 21...\n",
      "Training on subject 18...\n",
      "Training on subject 35...\n",
      "Training on subject 45...\n",
      "Training on subject 5...\n",
      "Training on subject 42...\n",
      "Training on subject 34...\n",
      "Training on subject 40...\n",
      "Training on subject 36...\n",
      "Training on subject 25...\n",
      "Training on subject 41...\n",
      "Training on subject 48...\n",
      "Training on subject 33...\n",
      "  Epoch   8: Train=10.5168, Val=9.8467, LR=0.001000\n",
      "Training on subject 5...\n",
      "Training on subject 14...\n",
      "Training on subject 25...\n",
      "Training on subject 30...\n",
      "Training on subject 6...\n",
      "Training on subject 48...\n",
      "Training on subject 16...\n",
      "Training on subject 42...\n",
      "Training on subject 35...\n",
      "Training on subject 24...\n",
      "Training on subject 34...\n",
      "Training on subject 40...\n",
      "Training on subject 17...\n",
      "Training on subject 21...\n",
      "Training on subject 4...\n",
      "Training on subject 36...\n",
      "Training on subject 45...\n",
      "Training on subject 15...\n",
      "Training on subject 20...\n",
      "Training on subject 47...\n",
      "Training on subject 44...\n",
      "Training on subject 7...\n",
      "Training on subject 18...\n",
      "Training on subject 43...\n",
      "Training on subject 13...\n",
      "Training on subject 9...\n",
      "Training on subject 1...\n",
      "Training on subject 8...\n",
      "Training on subject 19...\n",
      "Training on subject 31...\n",
      "Training on subject 41...\n",
      "Training on subject 12...\n",
      "Training on subject 33...\n",
      "Training on subject 2...\n",
      "Training on subject 27...\n",
      "Training on subject 26...\n",
      "Training on subject 39...\n",
      "Training on subject 28...\n",
      "Training on subject 3...\n",
      "Training on subject 37...\n",
      "  Epoch   9: Train=10.6694, Val=10.0098, LR=0.001000\n",
      "Training on subject 20...\n",
      "Training on subject 35...\n",
      "Training on subject 13...\n",
      "Training on subject 31...\n",
      "Training on subject 21...\n",
      "Training on subject 47...\n",
      "Training on subject 9...\n",
      "Training on subject 2...\n",
      "Training on subject 37...\n",
      "Training on subject 33...\n",
      "Training on subject 19...\n",
      "Training on subject 44...\n",
      "Training on subject 27...\n",
      "Training on subject 1...\n",
      "Training on subject 39...\n",
      "Training on subject 16...\n",
      "Training on subject 12...\n",
      "Training on subject 28...\n",
      "Training on subject 18...\n",
      "Training on subject 41...\n",
      "Training on subject 3...\n",
      "Training on subject 42...\n",
      "Training on subject 24...\n",
      "Training on subject 15...\n",
      "Training on subject 43...\n",
      "Training on subject 4...\n",
      "Training on subject 7...\n",
      "Training on subject 5...\n",
      "Training on subject 36...\n",
      "Training on subject 14...\n",
      "Training on subject 34...\n",
      "Training on subject 25...\n",
      "Training on subject 8...\n",
      "Training on subject 40...\n",
      "Training on subject 17...\n",
      "Training on subject 6...\n",
      "Training on subject 45...\n",
      "Training on subject 48...\n",
      "Training on subject 30...\n",
      "Training on subject 26...\n",
      "  Epoch  10: Train=10.7242, Val=9.7250, LR=0.001000\n",
      "Training on subject 34...\n",
      "Training on subject 36...\n",
      "Training on subject 6...\n",
      "Training on subject 13...\n",
      "Training on subject 9...\n",
      "Training on subject 8...\n",
      "Training on subject 2...\n",
      "Training on subject 24...\n",
      "Training on subject 15...\n",
      "Training on subject 43...\n",
      "Training on subject 4...\n",
      "Training on subject 7...\n",
      "Training on subject 20...\n",
      "Training on subject 25...\n",
      "Training on subject 12...\n",
      "Training on subject 33...\n",
      "Training on subject 45...\n",
      "Training on subject 28...\n",
      "Training on subject 37...\n",
      "Training on subject 19...\n",
      "Training on subject 17...\n",
      "Training on subject 44...\n",
      "Training on subject 16...\n",
      "Training on subject 40...\n",
      "Training on subject 47...\n",
      "Training on subject 21...\n",
      "Training on subject 42...\n",
      "Training on subject 31...\n",
      "Training on subject 14...\n",
      "Training on subject 3...\n",
      "Training on subject 27...\n",
      "Training on subject 35...\n",
      "Training on subject 26...\n",
      "Training on subject 5...\n",
      "Training on subject 18...\n",
      "Training on subject 48...\n",
      "Training on subject 30...\n",
      "Training on subject 39...\n",
      "Training on subject 41...\n",
      "Training on subject 1...\n",
      "  Epoch  11: Train=10.5588, Val=9.7884, LR=0.001000\n",
      "Training on subject 7...\n",
      "Training on subject 34...\n",
      "Training on subject 6...\n",
      "Training on subject 41...\n",
      "Training on subject 48...\n",
      "Training on subject 15...\n",
      "Training on subject 9...\n",
      "Training on subject 36...\n",
      "Training on subject 17...\n",
      "Training on subject 35...\n",
      "Training on subject 47...\n",
      "Training on subject 8...\n",
      "Training on subject 16...\n",
      "Training on subject 31...\n",
      "Training on subject 26...\n",
      "Training on subject 30...\n",
      "Training on subject 42...\n",
      "Training on subject 13...\n",
      "Training on subject 45...\n",
      "Training on subject 25...\n",
      "Training on subject 43...\n",
      "Training on subject 3...\n",
      "Training on subject 20...\n",
      "Training on subject 14...\n",
      "Training on subject 44...\n",
      "Training on subject 24...\n",
      "Training on subject 33...\n",
      "Training on subject 19...\n",
      "Training on subject 2...\n",
      "Training on subject 27...\n",
      "Training on subject 40...\n",
      "Training on subject 28...\n",
      "Training on subject 18...\n",
      "Training on subject 4...\n",
      "Training on subject 5...\n",
      "Training on subject 1...\n",
      "Training on subject 39...\n",
      "Training on subject 21...\n",
      "Training on subject 12...\n",
      "Training on subject 37...\n",
      "  Epoch  12: Train=10.7958, Val=10.1050, LR=0.001000\n",
      "Training on subject 24...\n",
      "Training on subject 47...\n",
      "Training on subject 2...\n",
      "Training on subject 15...\n",
      "Training on subject 6...\n",
      "Training on subject 17...\n",
      "Training on subject 26...\n",
      "Training on subject 42...\n",
      "Training on subject 34...\n",
      "Training on subject 16...\n",
      "Training on subject 1...\n",
      "Training on subject 14...\n",
      "Training on subject 33...\n",
      "Training on subject 21...\n",
      "Training on subject 8...\n",
      "Training on subject 39...\n",
      "Training on subject 44...\n",
      "Training on subject 27...\n",
      "Training on subject 20...\n",
      "Training on subject 37...\n",
      "Training on subject 18...\n",
      "Training on subject 36...\n",
      "Training on subject 19...\n",
      "Training on subject 3...\n",
      "Training on subject 28...\n",
      "Training on subject 9...\n",
      "Training on subject 5...\n",
      "Training on subject 48...\n",
      "Training on subject 43...\n",
      "Training on subject 40...\n",
      "Training on subject 45...\n",
      "Training on subject 25...\n",
      "Training on subject 30...\n",
      "Training on subject 35...\n",
      "Training on subject 13...\n",
      "Training on subject 4...\n",
      "Training on subject 7...\n",
      "Training on subject 41...\n",
      "Training on subject 31...\n",
      "Training on subject 12...\n",
      "  Epoch  13: Train=10.5032, Val=9.7275, LR=0.001000\n",
      "Training on subject 1...\n",
      "Training on subject 33...\n",
      "Training on subject 8...\n",
      "Training on subject 2...\n",
      "Training on subject 6...\n",
      "Training on subject 39...\n",
      "Training on subject 13...\n",
      "Training on subject 36...\n",
      "Training on subject 27...\n",
      "Training on subject 48...\n",
      "Training on subject 20...\n",
      "Training on subject 19...\n",
      "Training on subject 35...\n",
      "Training on subject 12...\n",
      "Training on subject 47...\n",
      "Training on subject 45...\n",
      "Training on subject 37...\n",
      "Training on subject 28...\n",
      "Training on subject 4...\n",
      "Training on subject 40...\n",
      "Training on subject 3...\n",
      "Training on subject 5...\n",
      "Training on subject 25...\n",
      "Training on subject 18...\n",
      "Training on subject 17...\n",
      "Training on subject 31...\n",
      "Training on subject 41...\n",
      "Training on subject 24...\n",
      "Training on subject 16...\n",
      "Training on subject 34...\n",
      "Training on subject 7...\n",
      "Training on subject 9...\n",
      "Training on subject 42...\n",
      "Training on subject 44...\n",
      "Training on subject 14...\n",
      "Training on subject 43...\n",
      "Training on subject 21...\n",
      "Training on subject 15...\n",
      "Training on subject 26...\n",
      "Training on subject 30...\n",
      "  Epoch  14: Train=10.4818, Val=9.8228, LR=0.001000\n",
      "Training on subject 25...\n",
      "Training on subject 41...\n",
      "Training on subject 40...\n",
      "Training on subject 28...\n",
      "Training on subject 36...\n",
      "Training on subject 42...\n",
      "Training on subject 13...\n",
      "Training on subject 47...\n",
      "Training on subject 16...\n",
      "Training on subject 27...\n",
      "Training on subject 31...\n",
      "Training on subject 17...\n",
      "Training on subject 34...\n",
      "Training on subject 3...\n",
      "Training on subject 14...\n",
      "Training on subject 37...\n",
      "Training on subject 20...\n",
      "Training on subject 4...\n",
      "Training on subject 30...\n",
      "Training on subject 44...\n",
      "Training on subject 45...\n",
      "Training on subject 24...\n",
      "Training on subject 26...\n",
      "Training on subject 1...\n",
      "Training on subject 43...\n",
      "Training on subject 18...\n",
      "Training on subject 8...\n",
      "Training on subject 21...\n",
      "Training on subject 6...\n",
      "Training on subject 39...\n",
      "Training on subject 35...\n",
      "Training on subject 5...\n",
      "Training on subject 2...\n",
      "Training on subject 33...\n",
      "Training on subject 48...\n",
      "Training on subject 15...\n",
      "Training on subject 7...\n",
      "Training on subject 19...\n",
      "Training on subject 12...\n",
      "Training on subject 9...\n",
      "  Epoch  15: Train=10.4458, Val=9.8557, LR=0.001000\n",
      "Training on subject 33...\n",
      "Training on subject 27...\n",
      "Training on subject 39...\n",
      "Training on subject 7...\n",
      "Training on subject 12...\n",
      "Training on subject 26...\n",
      "Training on subject 45...\n",
      "Training on subject 41...\n",
      "Training on subject 21...\n",
      "Training on subject 48...\n",
      "Training on subject 37...\n",
      "Training on subject 16...\n",
      "Training on subject 2...\n",
      "Training on subject 9...\n",
      "Training on subject 20...\n",
      "Training on subject 19...\n",
      "Training on subject 43...\n",
      "Training on subject 4...\n",
      "Training on subject 5...\n",
      "Training on subject 13...\n",
      "Training on subject 18...\n",
      "Training on subject 35...\n",
      "Training on subject 17...\n",
      "Training on subject 40...\n",
      "Training on subject 42...\n",
      "Training on subject 14...\n",
      "Training on subject 31...\n",
      "Training on subject 36...\n",
      "Training on subject 25...\n",
      "Training on subject 6...\n",
      "Training on subject 44...\n",
      "Training on subject 28...\n",
      "Training on subject 30...\n",
      "Training on subject 47...\n",
      "Training on subject 15...\n",
      "Training on subject 3...\n",
      "Training on subject 1...\n",
      "Training on subject 8...\n",
      "Training on subject 34...\n",
      "Training on subject 24...\n",
      "  Epoch  16: Train=10.4089, Val=9.6106, LR=0.001000\n",
      "Training on subject 12...\n",
      "Training on subject 43...\n",
      "Training on subject 24...\n",
      "Training on subject 13...\n",
      "Training on subject 7...\n",
      "Training on subject 33...\n",
      "Training on subject 36...\n",
      "Training on subject 44...\n",
      "Training on subject 25...\n",
      "Training on subject 35...\n",
      "Training on subject 17...\n",
      "Training on subject 2...\n",
      "Training on subject 6...\n",
      "Training on subject 27...\n",
      "Training on subject 3...\n",
      "Training on subject 16...\n",
      "Training on subject 21...\n",
      "Training on subject 48...\n",
      "Training on subject 15...\n",
      "Training on subject 1...\n",
      "Training on subject 30...\n",
      "Training on subject 19...\n",
      "Training on subject 26...\n",
      "Training on subject 39...\n",
      "Training on subject 9...\n",
      "Training on subject 5...\n",
      "Training on subject 20...\n",
      "Training on subject 40...\n",
      "Training on subject 37...\n",
      "Training on subject 41...\n",
      "Training on subject 31...\n",
      "Training on subject 4...\n",
      "Training on subject 42...\n",
      "Training on subject 47...\n",
      "Training on subject 34...\n",
      "Training on subject 28...\n",
      "Training on subject 14...\n",
      "Training on subject 45...\n",
      "Training on subject 8...\n",
      "Training on subject 18...\n",
      "  Epoch  17: Train=10.2708, Val=9.7162, LR=0.001000\n",
      "Training on subject 4...\n",
      "Training on subject 48...\n",
      "Training on subject 34...\n",
      "Training on subject 41...\n",
      "Training on subject 8...\n",
      "Training on subject 44...\n",
      "Training on subject 12...\n",
      "Training on subject 27...\n",
      "Training on subject 17...\n",
      "Training on subject 43...\n",
      "Training on subject 14...\n",
      "Training on subject 16...\n",
      "Training on subject 7...\n",
      "Training on subject 21...\n",
      "Training on subject 28...\n",
      "Training on subject 24...\n",
      "Training on subject 45...\n",
      "Training on subject 5...\n",
      "Training on subject 35...\n",
      "Training on subject 6...\n",
      "Training on subject 18...\n",
      "Training on subject 42...\n",
      "Training on subject 36...\n",
      "Training on subject 3...\n",
      "Training on subject 9...\n",
      "Training on subject 2...\n",
      "Training on subject 47...\n",
      "Training on subject 15...\n",
      "Training on subject 19...\n",
      "Training on subject 37...\n",
      "Training on subject 33...\n",
      "Training on subject 26...\n",
      "Training on subject 30...\n",
      "Training on subject 31...\n",
      "Training on subject 25...\n",
      "Training on subject 13...\n",
      "Training on subject 39...\n",
      "Training on subject 20...\n",
      "Training on subject 1...\n",
      "Training on subject 40...\n",
      "  Epoch  18: Train=10.6217, Val=9.9901, LR=0.001000\n",
      "Training on subject 27...\n",
      "Training on subject 12...\n",
      "Training on subject 9...\n",
      "Training on subject 48...\n",
      "Training on subject 40...\n",
      "Training on subject 25...\n",
      "Training on subject 47...\n",
      "Training on subject 6...\n",
      "Training on subject 30...\n",
      "Training on subject 36...\n",
      "Training on subject 13...\n",
      "Training on subject 35...\n",
      "Training on subject 16...\n",
      "Training on subject 24...\n",
      "Training on subject 21...\n",
      "Training on subject 44...\n",
      "Training on subject 17...\n",
      "Training on subject 18...\n",
      "Training on subject 19...\n",
      "Training on subject 5...\n",
      "Training on subject 28...\n",
      "Training on subject 33...\n",
      "Training on subject 45...\n",
      "Training on subject 14...\n",
      "Training on subject 20...\n",
      "Training on subject 3...\n",
      "Training on subject 42...\n",
      "Training on subject 26...\n",
      "Training on subject 2...\n",
      "Training on subject 43...\n",
      "Training on subject 31...\n",
      "Training on subject 7...\n",
      "Training on subject 8...\n",
      "Training on subject 37...\n",
      "Training on subject 34...\n",
      "Training on subject 41...\n",
      "Training on subject 39...\n",
      "Training on subject 1...\n",
      "Training on subject 4...\n",
      "Training on subject 15...\n",
      "  Epoch  19: Train=10.2531, Val=9.5186, LR=0.001000\n",
      "Training on subject 34...\n",
      "Training on subject 31...\n",
      "Training on subject 27...\n",
      "Training on subject 42...\n",
      "Training on subject 16...\n",
      "Training on subject 1...\n",
      "Training on subject 21...\n",
      "Training on subject 45...\n",
      "Training on subject 8...\n",
      "Training on subject 25...\n",
      "Training on subject 14...\n",
      "Training on subject 7...\n",
      "Training on subject 9...\n",
      "Training on subject 3...\n",
      "Training on subject 30...\n",
      "Training on subject 18...\n",
      "Training on subject 20...\n",
      "Training on subject 13...\n",
      "Training on subject 15...\n",
      "Training on subject 26...\n",
      "Training on subject 6...\n",
      "Training on subject 2...\n",
      "Training on subject 39...\n",
      "Training on subject 24...\n",
      "Training on subject 47...\n",
      "Training on subject 37...\n",
      "Training on subject 44...\n",
      "Training on subject 43...\n",
      "Training on subject 12...\n",
      "Training on subject 4...\n",
      "Training on subject 35...\n",
      "Training on subject 5...\n",
      "Training on subject 19...\n",
      "Training on subject 36...\n",
      "Training on subject 48...\n",
      "Training on subject 28...\n",
      "Training on subject 40...\n",
      "Training on subject 41...\n",
      "Training on subject 33...\n",
      "Training on subject 17...\n",
      "  Epoch  20: Train=10.0597, Val=9.6060, LR=0.000700\n",
      "Training on subject 24...\n",
      "Training on subject 7...\n",
      "Training on subject 18...\n",
      "Training on subject 37...\n",
      "Training on subject 28...\n",
      "Training on subject 33...\n",
      "Training on subject 48...\n",
      "Training on subject 3...\n",
      "Training on subject 27...\n",
      "Training on subject 8...\n",
      "Training on subject 40...\n",
      "Training on subject 4...\n",
      "Training on subject 12...\n",
      "Training on subject 36...\n",
      "Training on subject 47...\n",
      "Training on subject 42...\n",
      "Training on subject 13...\n",
      "Training on subject 45...\n",
      "Training on subject 17...\n",
      "Training on subject 41...\n",
      "Training on subject 15...\n",
      "Training on subject 30...\n",
      "Training on subject 19...\n",
      "Training on subject 2...\n",
      "Training on subject 16...\n",
      "Training on subject 31...\n",
      "Training on subject 14...\n",
      "Training on subject 9...\n",
      "Training on subject 43...\n",
      "Training on subject 1...\n",
      "Training on subject 6...\n",
      "Training on subject 44...\n",
      "Training on subject 26...\n",
      "Training on subject 34...\n",
      "Training on subject 20...\n",
      "Training on subject 5...\n",
      "Training on subject 25...\n",
      "Training on subject 21...\n",
      "Training on subject 39...\n",
      "Training on subject 35...\n",
      "  Epoch  21: Train=10.1865, Val=9.9011, LR=0.000700\n",
      "Training on subject 18...\n",
      "Training on subject 33...\n",
      "Training on subject 8...\n",
      "Training on subject 44...\n",
      "Training on subject 34...\n",
      "Training on subject 13...\n",
      "Training on subject 1...\n",
      "Training on subject 25...\n",
      "Training on subject 24...\n",
      "Training on subject 37...\n",
      "Training on subject 35...\n",
      "Training on subject 47...\n",
      "Training on subject 40...\n",
      "Training on subject 9...\n",
      "Training on subject 36...\n",
      "Training on subject 48...\n",
      "Training on subject 4...\n",
      "Training on subject 15...\n",
      "Training on subject 26...\n",
      "Training on subject 14...\n",
      "Training on subject 39...\n",
      "Training on subject 19...\n",
      "Training on subject 43...\n",
      "Training on subject 12...\n",
      "Training on subject 5...\n",
      "Training on subject 41...\n",
      "Training on subject 30...\n",
      "Training on subject 17...\n",
      "Training on subject 21...\n",
      "Training on subject 42...\n",
      "Training on subject 6...\n",
      "Training on subject 45...\n",
      "Training on subject 20...\n",
      "Training on subject 27...\n",
      "Training on subject 3...\n",
      "Training on subject 7...\n",
      "Training on subject 16...\n",
      "Training on subject 31...\n",
      "Training on subject 2...\n",
      "Training on subject 28...\n",
      "  Epoch  22: Train=10.2403, Val=9.6718, LR=0.000700\n",
      "Training on subject 15...\n",
      "Training on subject 24...\n",
      "Training on subject 7...\n",
      "Training on subject 19...\n",
      "Training on subject 12...\n",
      "Training on subject 30...\n",
      "Training on subject 33...\n",
      "Training on subject 16...\n",
      "Training on subject 45...\n",
      "Training on subject 1...\n",
      "Training on subject 17...\n",
      "Training on subject 20...\n",
      "Training on subject 36...\n",
      "Training on subject 34...\n",
      "Training on subject 14...\n",
      "Training on subject 42...\n",
      "Training on subject 2...\n",
      "Training on subject 39...\n",
      "Training on subject 35...\n",
      "Training on subject 5...\n",
      "Training on subject 8...\n",
      "Training on subject 21...\n",
      "Training on subject 3...\n",
      "Training on subject 47...\n",
      "Training on subject 9...\n",
      "Training on subject 37...\n",
      "Training on subject 28...\n",
      "Training on subject 13...\n",
      "Training on subject 4...\n",
      "Training on subject 18...\n",
      "Training on subject 27...\n",
      "Training on subject 43...\n",
      "Training on subject 44...\n",
      "Training on subject 25...\n",
      "Training on subject 26...\n",
      "Training on subject 40...\n",
      "Training on subject 48...\n",
      "Training on subject 41...\n",
      "Training on subject 31...\n",
      "Training on subject 6...\n",
      "  Epoch  23: Train=10.1220, Val=9.7458, LR=0.000700\n",
      "Training on subject 41...\n",
      "Training on subject 18...\n",
      "Training on subject 30...\n",
      "Training on subject 20...\n",
      "Training on subject 36...\n",
      "Training on subject 25...\n",
      "Training on subject 9...\n",
      "Training on subject 35...\n",
      "Training on subject 34...\n",
      "Training on subject 37...\n",
      "Training on subject 7...\n",
      "Training on subject 27...\n",
      "Training on subject 12...\n",
      "Training on subject 48...\n",
      "Training on subject 2...\n",
      "Training on subject 28...\n",
      "Training on subject 33...\n",
      "Training on subject 3...\n",
      "Training on subject 26...\n",
      "Training on subject 15...\n",
      "Training on subject 42...\n",
      "Training on subject 13...\n",
      "Training on subject 45...\n",
      "Training on subject 44...\n",
      "Training on subject 16...\n",
      "Training on subject 43...\n",
      "Training on subject 39...\n",
      "Training on subject 17...\n",
      "Training on subject 14...\n",
      "Training on subject 19...\n",
      "Training on subject 40...\n",
      "Training on subject 1...\n",
      "Training on subject 24...\n",
      "Training on subject 21...\n",
      "Training on subject 4...\n",
      "Training on subject 47...\n",
      "Training on subject 6...\n",
      "Training on subject 5...\n",
      "Training on subject 31...\n",
      "Training on subject 8...\n",
      "  Epoch  24: Train=10.2269, Val=9.6709, LR=0.000700\n",
      "Training on subject 30...\n",
      "Training on subject 12...\n",
      "Training on subject 7...\n",
      "Training on subject 39...\n",
      "Training on subject 33...\n",
      "Training on subject 31...\n",
      "Training on subject 4...\n",
      "Training on subject 44...\n",
      "Training on subject 40...\n",
      "Training on subject 26...\n",
      "Training on subject 27...\n",
      "Training on subject 18...\n",
      "Training on subject 15...\n",
      "Training on subject 24...\n",
      "Training on subject 6...\n",
      "Training on subject 8...\n",
      "Training on subject 1...\n",
      "Training on subject 21...\n",
      "Training on subject 3...\n",
      "Training on subject 35...\n",
      "Training on subject 2...\n",
      "Training on subject 47...\n",
      "Training on subject 45...\n",
      "Training on subject 43...\n",
      "Training on subject 25...\n",
      "Training on subject 16...\n",
      "Training on subject 20...\n",
      "Training on subject 14...\n",
      "Training on subject 34...\n",
      "Training on subject 41...\n",
      "Training on subject 5...\n",
      "Training on subject 48...\n",
      "Training on subject 37...\n",
      "Training on subject 36...\n",
      "Training on subject 42...\n",
      "Training on subject 28...\n",
      "Training on subject 17...\n",
      "Training on subject 13...\n",
      "Training on subject 9...\n",
      "Training on subject 19...\n",
      "  Epoch  25: Train=10.1947, Val=9.7291, LR=0.000700\n",
      "Training on subject 9...\n",
      "Training on subject 14...\n",
      "Training on subject 12...\n",
      "Training on subject 26...\n",
      "Training on subject 5...\n",
      "Training on subject 44...\n",
      "Training on subject 24...\n",
      "Training on subject 45...\n",
      "Training on subject 13...\n",
      "Training on subject 25...\n",
      "Training on subject 35...\n",
      "Training on subject 15...\n",
      "Training on subject 34...\n",
      "Training on subject 18...\n",
      "Training on subject 39...\n",
      "Training on subject 3...\n",
      "Training on subject 41...\n",
      "Training on subject 37...\n",
      "Training on subject 36...\n",
      "Training on subject 2...\n",
      "Training on subject 42...\n",
      "Training on subject 28...\n",
      "Training on subject 19...\n",
      "Training on subject 33...\n",
      "Training on subject 17...\n",
      "Training on subject 27...\n",
      "Training on subject 1...\n",
      "Training on subject 6...\n",
      "Training on subject 48...\n",
      "Training on subject 47...\n",
      "Training on subject 40...\n",
      "Training on subject 7...\n",
      "Training on subject 8...\n",
      "Training on subject 20...\n",
      "Training on subject 43...\n",
      "Training on subject 30...\n",
      "Training on subject 4...\n",
      "Training on subject 31...\n",
      "Training on subject 16...\n",
      "Training on subject 21...\n",
      "  Epoch  26: Train=10.1613, Val=9.6466, LR=0.000700\n",
      "Training on subject 7...\n",
      "Training on subject 30...\n",
      "Training on subject 45...\n",
      "Training on subject 19...\n",
      "Training on subject 9...\n",
      "Training on subject 18...\n",
      "Training on subject 20...\n",
      "Training on subject 16...\n",
      "Training on subject 34...\n",
      "Training on subject 3...\n",
      "Training on subject 13...\n",
      "Training on subject 39...\n",
      "Training on subject 14...\n",
      "Training on subject 31...\n",
      "Training on subject 21...\n",
      "Training on subject 25...\n",
      "Training on subject 36...\n",
      "Training on subject 8...\n",
      "Training on subject 27...\n",
      "Training on subject 24...\n",
      "Training on subject 28...\n",
      "Training on subject 41...\n",
      "Training on subject 17...\n",
      "Training on subject 15...\n",
      "Training on subject 6...\n",
      "Training on subject 42...\n",
      "Training on subject 1...\n",
      "Training on subject 37...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x10a00c510>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shawngibford/dev/qpkd/qpkd_env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on subject 47...\n",
      "Training on subject 5...\n",
      "Training on subject 12...\n",
      "Training on subject 2...\n",
      "Training on subject 26...\n",
      "Training on subject 4...\n",
      "Training on subject 44...\n",
      "Training on subject 48...\n",
      "Training on subject 40...\n",
      "Training on subject 35...\n",
      "Training on subject 43...\n",
      "Training on subject 33...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Run full analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     results, trainer, optimizer, pkpd_model, data_processor = \u001b[43mmain_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDiscrete dosing test failed - check implementation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmain_discrete\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Section 4: Training\u001b[39;00m\n\u001b[32m     33\u001b[39m trainer = DiscretePKPDTrainer(pkpd_model, data_processor)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m best_loss = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_loss < \u001b[32m10\u001b[39m:  \u001b[38;5;66;03m# Only proceed if training was successful\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Training successful (final loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 345\u001b[39m, in \u001b[36mDiscretePKPDTrainer.train_discrete\u001b[39m\u001b[34m(self, epochs, print_every)\u001b[39m\n\u001b[32m    342\u001b[39m train_loss = \u001b[38;5;28mself\u001b[39m.train_epoch(\u001b[38;5;28mself\u001b[39m.data_processor.train_subjects)\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m val_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval_subjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# Learning rate scheduling\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 321\u001b[39m, in \u001b[36mDiscretePKPDTrainer.validate_epoch\u001b[39m\u001b[34m(self, subject_list)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    320\u001b[39m     subject_data = \u001b[38;5;28mself\u001b[39m.data_processor.get_subject_data(subject_id)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss.item() > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m loss.item() < \u001b[32m50\u001b[39m:\n\u001b[32m    324\u001b[39m         val_loss += loss.item()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 224\u001b[39m, in \u001b[36mDiscretePKPDTrainer.compute_loss_discrete\u001b[39m\u001b[34m(self, subject_data)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Simulate with discrete dosing and get parameters used\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     solution, pk_params, pd_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulate_subject_discrete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdose_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_biomarker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# Check for numerical issues\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(solution).any() \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(solution).any():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 211\u001b[39m, in \u001b[36mDiscreteDosePKPDModel.simulate_subject_discrete\u001b[39m\u001b[34m(self, subject_id, covariates, dose_schedule, simulation_times, baseline_biomarker)\u001b[39m\n\u001b[32m    208\u001b[39m final_times = torch.tensor([current_time, max_time], dtype=torch.float32, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     final_solution = \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mode_system_no_dosing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdopri5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     solution_dict[\u001b[38;5;28mfloat\u001b[39m(max_time)] = final_solution[-\u001b[32m1\u001b[39m]\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/qpkd/qpkd_env/lib/python3.11/site-packages/torchdiffeq/_impl/odeint.py:80\u001b[39m, in \u001b[36modeint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[39m\n\u001b[32m     77\u001b[39m solver = SOLVERS[method](func=func, y0=y0, rtol=rtol, atol=atol, **options)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     solution = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m     event_t, solution = solver.integrate_until_event(t[\u001b[32m0\u001b[39m], event_fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/qpkd/qpkd_env/lib/python3.11/site-packages/torchdiffeq/_impl/solvers.py:34\u001b[39m, in \u001b[36mAdaptiveStepsizeODESolver.integrate\u001b[39m\u001b[34m(self, t)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m._before_integrate(t)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     solution[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_advance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m solution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/qpkd/qpkd_env/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:246\u001b[39m, in \u001b[36mRKAdaptiveStepsizeODESolver._advance\u001b[39m\u001b[34m(self, next_t)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m next_t > \u001b[38;5;28mself\u001b[39m.rk_state.t1:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m n_steps < \u001b[38;5;28mself\u001b[39m.max_num_steps, \u001b[33m'\u001b[39m\u001b[33mmax_num_steps exceeded (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m>=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m.format(n_steps, \u001b[38;5;28mself\u001b[39m.max_num_steps)\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28mself\u001b[39m.rk_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_adaptive_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrk_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     n_steps += \u001b[32m1\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _interp_evaluate(\u001b[38;5;28mself\u001b[39m.rk_state.interp_coeff, \u001b[38;5;28mself\u001b[39m.rk_state.t0, \u001b[38;5;28mself\u001b[39m.rk_state.t1, next_t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/qpkd/qpkd_env/lib/python3.11/site-packages/torchdiffeq/_impl/rk_common.py:337\u001b[39m, in \u001b[36mRKAdaptiveStepsizeODESolver._adaptive_step\u001b[39m\u001b[34m(self, rk_state)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# dtypes:\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# error_ratio.dtype == self.dtype\u001b[39;00m\n\u001b[32m    332\u001b[39m \n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m########################################################\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m#                   Update RK State                    #\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m########################################################\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accept_step:\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallback_accept_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     t_next = t1\n\u001b[32m    339\u001b[39m     y_next = y1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/qpkd/qpkd_env/lib/python3.11/site-packages/torchdiffeq/_impl/misc.py:11\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m      9\u001b[39m _all_callback_names = [\u001b[33m'\u001b[39m\u001b[33mcallback_step\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcallback_accept_step\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcallback_reject_step\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     10\u001b[39m _all_adjoint_callback_names = [name + \u001b[33m'\u001b[39m\u001b[33m_adjoint\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _all_callback_names]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m _null_callback = \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_unused_kwargs\u001b[39m(solver, unused_kwargs):\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unused_kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run discrete dosing test first\n",
    "    test_success = test_discrete_dosing()\n",
    "    \n",
    "    if test_success:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PROCEEDING TO FULL DISCRETE DOSING ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Run full analysis\n",
    "        results, trainer, optimizer, pkpd_model, data_processor = main_discrete()\n",
    "    else:\n",
    "        print(\"Discrete dosing test failed - check implementation\")\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qpkd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
